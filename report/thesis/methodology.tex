\chapter{Methodology}\label{meth}

Now that we have looked into the background and the previous work from the Clinical Decision Support (CDS) area,
let us proceed to formally defining our own retrieval system. 

The general setting is the following: we have a list of medical queries $\mathcal{Q}$ and a document collection $\mathcal{D}$.
For each query $Q \in \mathcal{Q}$, 
the retrieval system uses a scoring function $f:(Q,D)\rightarrow\mathbb{R}$ to compute the relevance score $f(Q,D)$ 
for each
document $D\in\mathcal{D}$, producing a list of scores $[f(Q,D_1), f(Q,D_2), \cdots, f(Q,D_{|\mathcal{D}|})]$,
sorted in descending order.
The rank $r_Q(D)$ of document $D$ is the position of $D$ in the list of sorted scores for query $Q$.
A performance function $\mathcal{P}:f\rightarrow\mathbb{R}$ measures the \emph{quality} of the rankings produced by
$f$, averaged over all the queries in $ \mathcal{Q}$.

Let us now define our scoring function.
Recall that the medical queries we are interested in have two components:
\begin{enumerate*}[label=\arabic*)]
 \item the \emph{query text}, containing the medical case report (patient history, current symptoms, any test results) and
 \item the \emph{query intent type}, that is, the generic clinical question the physician is interested in (diagnosis, test, or treatment).
\end{enumerate*}
We therefore represent a medical query $Q\in\mathcal{Q}$ as a tuple of text and intent type: \[Q = \left(Q_{\text{text}},\ Q_{\text{type}}\right).\]

Given a query $Q$, the retrieval system computes a relevance score for each document $D\in\mathcal{D}$.
The scoring process takes into account both the query text \emph{and} the query intent type. More precisely, we use the
$\textsf{SimilarityScore}(Q_{\text{text}}, D)$ and $\textsf{TypeClassifier}(Q_{\text{type}}, D)$ scoring functions to compute the relevance of the document to the query text and the
query intent type, respectively.

The final score is given by:
\begin{equation}\label{fusion}
  \textsf{FinalScore}(Q, D) = \textsf{Fusion}\left(\textsf{Similarity}(Q_{\text{text}}, D), \textsf{TypeClassifier}(Q_{\text{type}}, D)\right)
\end{equation}

Our scoring function is therefore a \emph{fusion} of two other scoring functions (Fig. \ref{diagram}).

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.25]{diagram.png}
  }
  \caption{Scoring function components}
  \label{diagram}
\end{figure}

In this paper, we study how to choose $\textsf{TypeClassifier}$ and $\textsf{Fusion}$ in order to maximize
\[
 \mathcal{P}(\textsf{Fusion}(\textsf{Similarity}, \textsf{TypeClassifier})) - \mathcal{P}(\textsf{Similarity})
\]
where $\mathcal{P}: f \rightarrow \mathbb{R}$ is the performance measure (e.g. Precision@10) of scoring function $f$.

In other words, we want to see \emph{if} and \emph{how} using the query intent types (via the
$\textsf{TypeClassifier}$) together with the \emph{baseline} \textsf{Similarity} scores
leads to an improvement in precision compared to only using the $\textsf{Similarity}$ scores.

\paragraph{Observation} It is important to note at this point that, even though scoring function \ref{fusion} does
compute the final scores,
the actual retrieval is performed in two steps: 
\begin{enumerate}
 \item we first retrieve the top $N$ documents according to their \textsf{Similarity} scores \emph{only};
 \vspace{-0.2cm}
 \item we then \emph{re-score} these $N$ documents according to formula \ref{fusion}.
\end{enumerate}
This helps us avoid cases where documents with little textual \textsf{Similarity} to the query text
have high \textsf{Fused} scores due to very high \textsf{TypeClassifier} scores masking out potentially low \textsf{Similarity} scores.
We therefore use step 1 from above as a ``filter'' to make sure the re-scored documents are still the ones that had high textual
\textsf{Similarity} to begin with.

Let us now look at each of the three components in formula \ref{fusion} individually.

\section{Text Similarity}
For computing the similarity between a query and a document, we use the well-known BM25 function\cite{bm25}.
We therefore have:
\begin{equation}\label{bm25-formula}
 \textsf{Similarity}(Q_{\text{text}}, D) = \textsf{BM25}(Q_{\text{text}},D)% = \displaystyle\sum_{term\in Q.text} \text{IDF}(term)\cdot\displaystyle\frac{freq(term,D)\cdot(k_1+1)}{freq(term,D)+k_1\cdot\left(1-b+b\cdot\frac{|D|}{avgDocLen}\right)}
\end{equation}

\section{Intent Type Scoring}\label{intent-scoring}
Intent type scoring should tell us whether a particular document is useful for the intent type of the query 
(diagnosis, test or treatment). There are various approaches to do this, but in this paper we mainly look at
binary classification and keyword-counting.

For each intent type $T$ we construct a machine-learning classifier and a keyword-counter in the following way:
\begin{itemize}
 \item the ML-classifier computes $\textsf{ClassifierScore}(T,D)$ which measures how much document $D$ conforms to type $T$;
 \item the keyword counter computes a score which measures the frequency of certain keywords related to type $T$ in document $D$:
 \begin{equation}\label{basic-formula}
  \textsf{KeywordCounterScore}(T,D) = \min\left(1,\,\, \displaystyle\frac{\displaystyle\sum_{w\in \textsf{Keywords}(T)} \textsf{count}(w,D)}{\textsf{cutoff}}\right),
 \end{equation}
  where $\textsf{cutoff} > 0$ is determined empirically.
\end{itemize}

The final intent type score is a linear combination of the two scores:
\begin{equation}\label{clf-combine-formula}
 \textsf{TypeClassifierScore} = \alpha\cdot\textsf{ClassifierScore} + (1-\alpha)\cdot\textsf{KeywordCountScore},
\end{equation}
where $\alpha\in (0,1)$.

The purpose of the keyword counter is to ``double-check'' the classifier predictions and potentially correct its mistakes.
This would be the case, for instance,
if a document with many diagnosis-related words is misclassified as a non-diagnosis document,
or, vice-versa, if a document with no diagnosis-related words is misclassified as a diagnosis document.
In these cases, the keyword counter would either increase or decrease the overall score, moving it closer to the truth.


While the keyword counter is straightforward, the classifier is a more complex component. In this paper, we investigate 
a variety of classifiers, ranging from classical ones like logistic regression and support vector machines, to more modern
ones, like neural networks. Let us give a short overview:

\begin{itemize}
 \item \textbf{Support Vector Machine (SVM)} classifiers ---
%  are a popular method for binary classification. SVM classifiers work by
% trying to \emph{separate} the feature hyperplane by a \emph{support vector}
% that maximizes the distance to the nearest points of the two classes.
% In this paper, 
we use multiple loss functions, such as hinge loss,
squared hinge loss, $\epsilon$-insensitive loss, squared loss, zero/one loss and error-rate loss. 
We also use SVM classifiers developed in \cite{svmperf3}, \cite{svmperf1} and
\cite{svmperf2}, which use multivariate,
nonlinear loss functions such as $F_1$ loss, Precision@K loss, Recall@K loss and ROC-Area loss;
\item \textbf{Ridge} \cite{scikit} --- similar to SVM with squared loss;
\item \textbf{Logistic regression} \cite{scikit} --- similar to SVM with the logistic function loss;
\item \textbf{Perceptron} \cite{scikit};
\item \textbf{Passive Aggressive} \cite{passiveAggr} --- similar to Perceptron, but with an update rule
  based on the hinge loss;
\item \textbf{Naive Bayes} \cite{ir-intro};
\item \textbf{Multilayer Perceptron} \cite{scikit};
\item \textbf{Convolutional Neural Networks} --- based on the construction from \cite{cnn}, but using an additional
hidden layer before the last (fully connected) layer and using fixed, \texttt{word2vec}-learned word embeddings \cite{w2v}.
\end{itemize}

\section{Fusion of Text Similarity and Intent Type Scores}
Now that we have the similarity score and the intent type score, we need to fuse them to produce the final score.
We employ two main methods of fusion: unsupervised and supervised.

\subsection{Unsupervised Fusion}
Let $Q$ and $D$ be a query and a document and let $S=\textsf{SimilarityScore}(Q_{\text{text}}, D)$
and $C=\textsf{TypeClassifierScore}(Q_{\text{type}}, D)$, where \textsf{SimilarityScore} and \textsf{TypeClassifierScore}
are defined as in equations \ref{bm25-formula} and \ref{clf-combine-formula}, respectively.

Furthermore, let $r_S$ and $r_C$ be the ranks of $D$ in the ordered list of scores given by the
\textsf{Similarity} and \textsf{TypeClassifier} scoring functions for query $Q$ over the top $N$ retrieved documents,
respectively.

We separately look into three unsupervised fusion methods:
\begin{itemize}

 \item \textbf{Weighted Linear Combination} --- the fused score is given by: 
 \begin{equation} \label{interp-formula}
 \textsf{LinComb}(S, C) = \lambda S + (1-\lambda) C, \text{ where } \lambda \in (0,1);
 \end{equation}

\item \textbf{Weighted Reciprocal Rank Fusion} \cite{rrf} --- the fused score is given by:
 \begin{equation}\label{rrf-formula}
  \textsf{RRF}(r_S, r_C) =  \displaystyle\frac{\lambda}{k + r_S} + \displaystyle\frac{1- \lambda}{k + r_C},  \text{ where } \lambda \in (0,1)
  \text{ and } k>0;
 \end{equation}
 
 \item \textbf{Weighted Borda Fusion} \cite{borda} ---  the fused score is given by:
 \begin{equation}\label{borda-formula}
   \textsf{Borda}(r_S, r_C) =  \displaystyle\frac{1}{\lambda \cdot r_S + (1-\lambda)\cdot r_C},  \text{ where } \lambda \in (0,1).
 \end{equation}
  \end{itemize}

The weight $\lambda$ measures the contribution of the baseline \textsf{Similarity} score and
is determined empirically for each of the three methods. For $\lambda=1$, the fused score is equal to the baseline,
while for $\lambda=0$, the fused score is equal to the type classifier score.

\subsection{Supervised Fusion}\label{sup-fusion}
Let us now briefly list the various supervised fusion methods that we use:
\begin{itemize}
 \item {Ada Rank} \cite{adaRank};
 \item {Rank Net} \cite{rankNet};
 \item {Coordinate Ascent} \cite{coordAscent};
 \item {MART} \cite{mart};
 \item {Lambda MART} \cite{lambda-mart};
 \item {Rank Boost} \cite{rankBoost};
  \item {Linear Regression}.
\end{itemize}

These algorithms are called \textbf{Learning to Rank} algorithms --
they take as input a list of features for each query-document pair $(Q,D)$ in the training set
and learn how to rank documents based on the
relevance judgments of those pairs. At test time, they apply the learned model to rerank other query-document pairs.

In our case, we use the $\textsf{SimilarityScore}(Q_{\text{text}},D)$ and $\textsf{TypeClassifierScore}(Q_{\text{type}}, D)$ 
as features for each $(Q,D)$-pair.

We have now given a complete theoretical definition of our retrieval system. We described the problem setting,
the two main components of the scoring function --- the baseline similarity score and the classifier score ---
and also the various reranking algorithms used for fusing the two scores.

Figure \ref{diagram2} shows the entire system architecture with all the main components.

\begin{figure}
\centerline{
  \includegraphics[scale=0.25]{diagram2.png}
  }
  \caption{System architecture.}
  \label{diagram2}
\end{figure}

Let us now move forward to the implementation part.


