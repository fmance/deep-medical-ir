\chapter{Experimental Results}

Let us now present the experimental results of applying our system formally defined in Chapter \ref{meth} and implemented in Chapter 
\ref{impl}
to the TREC 2014 and 2015 Clinical Decision Support track.
We first discuss the evaluation metric and the classifier parameters,
then we move on to discussing the results for the unsupervised fusion methods (Linear Combination, RRF and Borda)
and then the results for the supervised fusion methods (Learning to Rank). We then do an overall comparison between the best results from each strategy
and, finally, we compare our overall best with the results from \cite{choi}.

\section{Query runs}\label{runs} Before we begin, let us recall that we have four \emph{runs} with 30 queries each:
\begin{itemize}
 \item using the query \emph{summaries} from \emph{2014};
 \item using the query \emph{descriptions} from \emph{2014};
 \item using the query \emph{summaries} from \emph{2015};
 \item using the query \emph{descriptions} from \emph{2015};
\end{itemize}

We present a set of results for each of these runs and also three \emph{mean results}:
\begin{itemize}
 \item the mean for 2014 (summaries and descriptions);
 \item the mean for 2015 (summaries and descriptions);
 \item the overall mean over all four runs.
\end{itemize}

\section{Evaluation Metric}
To evaluate the quality of our rankings, we use the precision at 10 (P@10) metric, which counts for each query the percentage of
relevant results from the top 10 retrieved results. For example, if 4 documents of the top 10 retrieved results are relevant,
then P@10 is equal to 40\%. The P@10 is then averaged over all queries in a run.

As discussed in Section \ref{qrels}, the ground truths --- relevance judgements --- are provided by medical experts
and stored in the (\emph{qrels}) files.

\section{Classifier Parameters}
Table \ref{params} lists the 17 classifiers we use and their configurations. 
We empirically determine
the 
parameters by varying them in a cerain range (for example, the regularization parameters
in the $[10^{-4},\cdots,10^4]$ in steps of powers of 10) and choosing the ones
that give the
best P@10 improvement averaged over all four categories.

For the Multilayer Perceptron classifier, we achieved the best results by using two layers of size 50 and 5 respectively
(we ranged the layer sizes from 5 to 100 and the number of layers from 1 to 3).

For the Convolutional Neural Networks classifier, we achieved the best results by
using 100 convolution filters of size 3, 4 and 5, respectively and 50 nodes in last hidden layer; the word-embedding size is 100
(we ranged the number of filters from 50 to 150 and the number of nodes in the last hidden layer from 0, i.e., no hidden layer, to 75).

The weight $\alpha$ of the Keyword Counter score to the overall classifier score (formula \ref{clf-combine-formula}) is set to 0.4 and
the \textsf{cutoff} is set to 4.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Best classifier parameters found}
\label{params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Classifier}             & \textbf{Loss function}  & \textbf{Penalty} & \textbf{Regularization parameter}                             &  &  \\ \midrule
SVM               		& Precision@K		 & L2               & $10^{-3}$                                                 &  &  \\
SVM               		& Recall @K		 & L2               & $10^{-3}$                                               &  &  \\
SVM           			& ROC area		 & L2               & $10^{-3}$                                                &  &  \\
SVM           			& Error-rate		& L2               & $1$                                                     &  &  \\
SVM                 		& F1			& L2               & $1$                                                     &  &  \\
SVM           			& Zero/one		& L2               & $10^{-2}$                                                  &  &  \\
SVM      			& $\epsilon$-insensitive	& Elasticnet       & $10^{-2}$                                              &  &  \\
SVM              		& Hinge 		& L2               & $10^{-3}$                                                 &  &  \\
SVM       			& Squared hinge		& L2               & $10^{-3}$                                                 &  &  \\
SVM             		& Squared		& Elasticnet       & $10^{-2}$                                              &  &  \\
Ridge               		& Squared		 & L2               & $10^{-4}$                                             &  &  \\
Logistic Regression             & Log			& L2               & $10^{-1}$                                                   &  &  \\
Passive Aggressive		& Hinge 		&             & $10^{-2}$                                                  &  &  \\
Perceptron                      &			& Elasticnet             & $10^{-4}$                                            &  &  \\
Multilayer Perceptron          & Log			& Elasticnet       & $10^{-6}$                                                        &  &  \\
Neural Networks                 & Cross-entropy		& L2               & $10^{-4}$ &  &  \\ 
Naive Bayes                    	& 			 &              & $10^{-3}$ (Laplace smoothing)                            &  &  \\\bottomrule
\end{tabular}%
}
\end{table}

\section{Unsupervised Fusion Results}
Let us now present the results for the unsupervised fusion methods. 

For each of the four runs listed in Section \ref{runs} and for each of the seventeen classifiers listed in Table \ref{params}, we perform a fusion of scores
between the baseline BM25 similarity scores and the classifier scores. 
We therefore produce seventeen sets of results for each of the four runs,
one set of results per classifier.
We then average for each classifier the results of the corresponding four runs, producing seventeen \emph{average results}.
This shows which classifier leads to the highest P@10 improvement for that fusion method.

Furthermore, we look at the contribution of the keyword counter towards the overall P@10 improvement. More precisely,
we look at what the improvement is \emph{with} (default) versus \emph{without} using the keyword counter, that is, using $\alpha=0.4$
versus using $\alpha=0$, respectively, in formula \ref{clf-combine-formula}.

Lastly, using the best classifier, we show for each run the improvement for the 30 queries individually.

\subsection{Linear Combination Results}
We begin by showing the results of the Linear Combination fusion method (formula \ref{interp-formula}) in table \ref{interpolation-res}.
The weight $\lambda$ is set to $0.7\pm 0.02$.

The first line shows the \emph{baseline} P@10 for each of the four runs.
The next lines show the \emph{absolute} P@10 \emph{improvements} for each classifier.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Linear Combination} --- absolute P@10 improvements.}
\label{interpolation-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\\midrule
Baseline                             & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
SVM (with Precision@K loss)                                 & +8                       & +9                          & +\textbf{4}              & +3.33                       & +\textbf{6.08}    \\ 
SVM (with Rec@K loss)                                    & +\textbf{8.33}           & +\textbf{10}                & +3                       & +2.67                       & +6                \\ 
Ridge                                          & +7                       & +9                          & +3.33                    & +\textbf{4}                 & +5.83             \\ 
SVM (with ROC area loss)                                    & +7.67                    & +9.67                       & +2                       & +3.67                       & +5.75             \\ 
SVM (with Error-rate loss)                                 & +8                       & +8.67                       & +3                       & +3                          & +5.67             \\ 
SVM (with Squared hinge loss)                           & +7                       & +9                          & +3                       & +3.67                       & +5.67             \\ 
Logistic Regression                            & +7.67                    & +8.67                       & +3                       & +3                          & +5.59             \\ 
SVM (with Squared loss)                               & +7.67                    & +9.33                       & +3.33                    & +1.67                       & +5.5              \\ 
SVM (with F1 loss)                                       & +7                       & +8.67                       & +2.67                    & +3.33                       & +5.42             \\ 
SVM (with Zero/one loss)                                        & +7                       & +8.67                       & +3                       & +3                          & +5.42             \\ 
Passive Aggressive                             & +6                       & +8                          & +2.33                    & +4                          & +5.08             \\ 
SVM (with $\epsilon$-insensitive loss)                            & +6.33                    & +7.67                       & +2.67                    & +3                          & +4.92             \\ 
SVM (with Hinge loss)                                     & +5.33                    & +8                          & +2                       & +3.67                       & +4.75             \\ 
Multilayer Perceptron                           & +7.67                    & +8                          & +1.67                    & +1                          & +4.59             \\ 
Neural Networks                               & +3                       & +5.67                       & +2.33                    & 0                          & +2.75             \\ 
Perceptron                                      & +4.33                    & +4.33                       & +0.33                    & 0                          & +2.25             \\ 
Naive Bayes                                      & +3                       & +4                          & +1                       & -1                         & +1.75             \\ \bottomrule
\end{tabular}%
}
\end{table}

The best rerankers are the ones using SVM classifiers, with the top one giving an overall improvement of over 6 percentage
points in P@10. For 2014, the best classifier was SVM with Rec@K loss (8.33\% and 10\% improvements), 
and for 2015 they were SVM with Precision@K loss and Ridge (4\% improvements).

In particular, for summaries 2014, the top reranked result is $31.67\%+8.33\% = 40\%$, which is better than the top automatic submission
from TREC 2014 (39\%).

Generally, the improvements for 2014 are at least 
twice as high as the ones for 2015, but this may be due to the lower baseline precisions from 2014 compared to 2015.

Looking more closely, we can see that the top 10 classifiers are not more than 0.7\% worse off compared to the best,
which means that any one of them could be trusted to work well with other queries.

However, the bottom three classifiers are more modest, yielding improvements less than 3\%.
In the case of neural networks, the problem might be the small size of the training data,
while the Perceptron and Bayes classifiers are too naive for this task.
Nevertheless, they still give some moderate improvements for 2014 and Summaries 2015.

\subsection{Reciprocal Rank Fusion Results}
Let us now show the results of the Reciprocal Rank fusion method (formula \ref{rrf-formula}) in table \ref{rrf-res}.
The weight $\lambda$ is set to $0.6\pm 0.02$ and $k$ is set to 60.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Reciprocal Rank Fusion} --- absolute P@10 improvements.}
\label{rrf-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
                                    & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\ \midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule

SVM (with Squared loss)                               & +5                       & +7.33                       & +3.67                    & +\textbf{3}                          & +\textbf{4.75}             \\ 
SVM (with Rec@K loss)                                                                      & +5                       & +\textbf{7.67}                       & +{3}              & +2                          & +{4.42}    \\ 
Passive Aggressive                                       & +\textbf{5.33}           & +5.67              & +2.33                    & +2.67                       & +4                \\ 
SVM (with Precision@K loss)                                                                 & +3.67                    & +7                          & +3.33                    & +{1.67}              & +3.92             \\ 
SVM (with F1 loss)                                                                          & +4.33                    & +6.33                       & +2.67                    & +2.33                       & +3.92             \\ 
Logistic Regression                                                      & 3.67                    & +7                          & +2.67                    & +2                          & +3.84             \\ 
SVM (with Hinge loss)                                                                        & +4                       & +5.67                       & +2.67                    & +\textbf{3}                          & +3.84             \\ 
SVM (with Error-rate loss)                                                                    & +4.33                    & +6                          & +3.33                    & +1.67                       & +3.83             \\ 
SVM (with Squared hinge loss)                                                              & +3.67                    & +6.33                       & +2.33                    & +2.67                       & +3.75             \\ 
Ridge                                                             & +3.33                    & +7                          & +2.33                    & +2                          & +3.67             \\ 
SVM (with $\epsilon$-insensitive loss)                         & +4.33                    & +6.67                       & +1.67                    & +1.67                       & +3.59             \\ 
SVM (with ROC area loss)                                                                    & +3.33                    & +7                          & +2.33                    & +1.67                       & +3.58             \\ 
SVM (with Zero/one loss)                                                                        & +4                       & +5.67                       & +2.33                    & +2.33                       & +3.58             \\ 
Multilayer Perceptron								& +4 & +6.33 & +2 & +0 & +3.08 \\
Naive Bayes                                                                      & +5                       & +5.67                       & +1.33                    & -1                         & +2.75             \\ 
Neural Networks                                                                   & +3.67                    & +4.67                       & 0                       & +1                          & +2.34             \\ 
Perceptron                                                              & +2                       & +3                          & +1                       & 0                          & +1.5              \\ \bottomrule
\end{tabular}%
}
\end{table}

Similar to the linear combination method, SVM classifiers generally work best,
although the top one now uses the squared loss instead of Precision@K loss (which takes fourth place instead).

RRF, however, gives lower improvements, with the highest one being 4.75\%
compared to 6\% for linear combination.
This may be due to the fact that RRF loses some information by only using the ranks and not the actual scores. On the other hand, RRF
is easier and more convenient to compute, since no score scaling is needed.


\subsection{Borda Fusion Results}
We show the results of the Borda fusion method (formula \ref{borda-formula}) in table \ref{borda-res}.
The weight $\lambda$ is set to $0.6\pm 0.05$.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Borda Fusion} --- absolute P@10 improvements.}
\label{borda-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
                                       & Summaries 2014 & Descriptions 2014 & Summaries 2015 & Descriptions 2015 & Average       \\\midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
                                   
SVM (with Squared loss)                               & +\textbf{4.67}  & +\textbf{8.67}     & +2.33           & +\textbf{1.33}     & +\textbf{4.25} \\
SVM (with Precision@K loss)                            & +4              & +9                 & +1.67           & +\textbf{1.33}     & +4             \\
SVM (with Rec@K loss)                                 & +4.33           & +7.67              & +3.33           & +0.67              & +4             \\
Logistic Regression                                    & +4.33           & +8                 & +2              & +\textbf{1.33}     & +3.92          \\
SVM (with Error-rate loss)                             & +4.33           & +6                 & +3.33           & +1                 & +3.67          \\
SVM (with F1 loss)                                     & +4              & +5.67              & +4              & +1                 & +3.67          \\
SVM (with Squared hinge loss)                           & +3              & +8                 & +2              & +\textbf{1.33}     & +3.58          \\
SVM (with Zero/one loss)                               & +3.67           & +5.67              & +\textbf{3.67}  & +1                 & +3.5           \\
Ridge                                                   & +3              & +8                 & +1.33           & +1                 & +3.33          \\
SVM (with ROC area loss)                                & +3.67           & +8                 & +1.33           & +0.33              & +3.33          \\
SVM (with $\epsilon$-insensitive loss)                   & +3.33           & +7                 & +1.33           & +1                 & +3.17          \\
Passive Aggressive                                       & +3.67           & +5                 & +1.67           & +1                 & +2.84          \\
SVM (with Hinge loss)                                    & +2              & +6.33              & +0.67           & +1                 & +2.5           \\
Multilayer Perceptron                                 & +2.67           & +6.33              & +1              & -1                & +2.25          \\
Naive Bayes                                              & +3.33           & +6.33              & +0.33           & -3                & +1.75          \\
Neural Networks                                         & +1              & +4.33              & -1             & -1                & +0.83          \\
Perceptron                                              & +1.33           & +2                 & +0.67           & +-2.33             & +0.42         \\\bottomrule
\end{tabular}%
}
\end{table}

Again, SVM classifiers work best, the top one using the squared loss, like RRF. 
However, the results for Borda are generally lower than for RRF by around 0.5\%. This is mostly due
to poorer results for 2015 (1.83\% vs. 3.33\% for the top result).
On the other hand, Borda works slightly better for 2014 (6.67\% vs 6.16\% for the top result).

Overall, the results of the three fusion methods show that \[\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}.\]

\subsection{Relation between weight $\lambda$ and improvement}
Let us now investigate the relation between the weight $\lambda$ and the improvement in P@10 for each of the three
fusion methods, when using their best classifier (i.e., SVM with Precision@K loss for linear combination
and SVM with squared loss for RRF and Borda).
Recall from formulas \ref{interp-formula}, \ref{rrf-formula} and \ref{borda-formula} that the weight $\lambda$ is the 
weight of the baseline BM25 score and $1-\lambda$ is the weight of the classifier score.

Figure \ref{lambda-vary} shows for each of the three methods
how the improvement in P@10 behaves as the weight $\lambda$ is varied between 0 and 1 in steps of 0.01.

\begin{figure}[h!]

\begin{subfigure}[b]{\textwidth}
 \centerline {
  \includegraphics[scale=0.22]{../../ranking/plots/interpolation/all/all.SVMPerf.04.0.001.hedges.png}
  }
%   \vspace{-0.1cm}
%   \caption{P@10 improvements using linear combination and SVM with Precision@K loss function.}
  \label{interp-weight}
\end{subfigure}
%  \vspace{0.5cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/rrf/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using RRF and SVM with squared loss function.}
  \label{rrf-weight}
\end{subfigure}
% \vspace{0.2cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/borda/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using Borda fusion and SVM with squared loss function.}
  \label{borda-weight}
\end{subfigure}
 
 \caption{Relation between weight $\lambda$ and improvement}
 \label{lambda-vary}
 
\end{figure}

The first thing we see in Figure \ref{lambda-vary} is that the improvements generally happen for $\lambda \in [0.5, 0.8]$.
As also reflected in tables \ref{interpolation-res}, \ref{rrf-res} and \ref{borda-res}, the 2014 improvements are much higher than
the 2015 ones. 

\paragraph{Observation} Intrestingly, for Descriptions 2014, using $\lambda = 0$ gives better results
than using $\lambda=1$. This does not mean, however, that for 2014, using the classifier scores 
works better than using the BM25 scores, but rather that
\emph{re-sorting} the top 100 BM25 results by their classifier scores is better. 
We still need the BM25 retrieval step to work as a ``filter'',
since otherwise the top retrieved documents would be the ones with the highest classifier scores out of the whole document collection, 
regardless of whether
they have any \emph{textual} similarity to the query --- this is also discussed in Chapter \ref{meth}.
In any case,  however, this peculiarity only happens for 2014; for 2015 the improvements happen only when $\lambda>0.5$,
and even for 2014, the highest improvements happen also when $\lambda>0.5$.

Moving forward with the analysis of Figure \ref{lambda-vary}, we see that
the improvements for 2014 and 2015 are more correlated with each other when using linear combination than when using
RRF or Borda. By this we mean that the two weights that give the peak average improvements for 2014 and 2015 are closer to each other
when using linear combination (both around $0.7$) than they are when using RRF or Borda ($0.4$ vs $0.6$ for 2014 and 2015, respectively).
This ``mismatch'' might also contribute to the lower average peak precision for RRF and Borda ($<4.8\%$) 
vs. Linear Combination ($6\%$), despite the fact that they sometimes 
give higher peak precisions for 2014 (e.g. peak Descriptions 2014 improvement 
is $\sim10\%$ for Borda vs. only $\sim9\%$ for Linear combination). However, the 2015 improvements are always higher for
Linear Combination.

Looking beyond these granular differences and more at the overall picture, we can safely conclude
that choosing any of the three unsupervised fusion methods and any weight $\lambda$ between 0.6 and 0.8 would give
a solid improvement over the baseline.

\subsection{Contribution of the Keyword Counter}
Recall from formula \ref{clf-combine-formula} that the classifier score is actually 
a linear combination of the machine learning classifier score (e.g., SVM) and the keyword counter score. The idea, 
explained in Section \ref{intent-scoring}, 
was that they keyword counter would double-check the classifier and maybe correct its predictions by
increasing or lowering the scores depending on the frequency of certain keywords.

Figure \ref{basic-effect-plot}
confirms the fact that the keyword counter helps. The average improvement for linear combination is more than 1\% higher
when using it than when not using it and the $\lambda$--P@10 curve is generally wider.

For RRF and Borda the effect is less pronounced --- probably because they only use the ranks and not the scores ---
but is still there nonetheless.

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/basic-vs-non-basic.png}
  }
  \caption{Contribution of using the Keyword Counter.}
  \label{basic-effect-plot}
\end{figure}

\subsection{Improvement per query}
We conclude our analysis of the unsupervised fusion methods by examining the improvement per query.

Figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query} show the change (increase or decrease) in P@10 for each query,
using Linear Combination, RRF and Borda fusion, respectively. 
We show the change for each of the four runs \{Summaries, Descriptions\} $\times$ \{2014, 2015\}.

Table \ref{change-per-query-table} shows a summary of the number of queries that got better or worse after the reranking.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Number of better/worse queries.}
\label{change-per-query-table}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
			& \textbf{Better 2014} & \textbf{Worse 2014} & \textbf{Better 2015} & \textbf{Worse 2015} \\ \midrule
Linear Combination 	& 34                   & 4                   & 24                   & 10                  \\
Reciprocal Rank Fusion  & 27                   & 3                   & 22                   & 10                  \\
Borda fusion       	& 30                   & 9                   & 24                   & 20                  \\ \bottomrule
\end{tabular}%
}
\end{table}

This reinforces the idea that $\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}$
and that 2014 is easier to rerank than 2015.

Looking more closely at figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query}, it is clear that
the treatment queries (21-30) fared better than the diagnosis queries (1-10) which fared better than the test queries (11-20).
Table \ref{change-per-query-table-lincomb} lists this distribution of query changes per intent
type for Linear Combination.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Linear Combination - Number of better/worse queries per intent type.}
\label{change-per-query-table-lincomb}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
                  & \textbf{Better 2014} & \textbf{Worse 2014} & \textbf{Better 2015} & \textbf{Worse 2015} \\ \midrule
Treatment Queries & 15                   & 0                   & 10                   & 1                   \\ 
Diagnosis Queries & 10                   & 2                   & 8                    & 5                   \\
Test Queries      & 9                    & 2                   & 6                    & 4                   \\\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SVMPerf.04.0.001.hedges.interpolation.png}
  }
  \caption{Linear Combination - P@10 change per query.}
  \label{interp-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.rrf.png}
  }
  \caption{RRF - P@10 change per query.}
  \label{rrf-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.borda.png}
  }
  \caption{Borda Fusion - P@10 change per query.}
  \label{borda-query}
\end{figure}


\section{Supervised reranking}

We now present the results of the supervised reranking strategies. We use several learning to rank algorithms, we present the ones that have the best results.
As described earlier, when reranking the 2014 queries, we use the 2015 ones as training set and vice versa.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Parameters}
\label{my-label}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|l@{}}
\toprule
Algorithm         & \textbf{Parameters}                                                             \\\midrule
RankNet           & 40 iterations, 1 hidden layer, 10 nodes/layer                                   \\
RankBoost         & 40 iterations                                                                   \\
AdaRank           & 10 iterations, max number of times a feauture can be consecutively selected: 2  \\
Coordinate Ascent      & 20 iterations, Regularization parameter: $10^{-3}$                  \\
MART              & 100 trees, 10 leaves                                                            \\
Lambda MART       & 60 trees, 10 leaves                                                             \\
Linear Regression & Regularization parameter: $10^{-10}$      				\\\bottomrule                        
\end{tabular}%
}
\end{table}

Here are the overall results:

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\caption{Learning to Rank results.}
\label{l2r-res}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
\toprule
Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule

Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule
Baseline          & \multicolumn{2}{c|}{31.67}          & \multicolumn{2}{c|}{25}              & \multicolumn{2}{c|}{38.67}     & \multicolumn{2}{c|}{35}     &32.59         \\ \midrule

Linear Regression & \textbf{+7}       & +6.67             & \textbf{+8.33}   & +7.67              & +2.67       & +1.67    	& +1          & \textbf{+2.33}    & +\textbf{5.08}  \\
AdaRank           & +6.33             & +3.33             & +8               & +6.67              & \textbf{+3} & +1.67         & +1.33       & \textbf{+2.33}    & +4.91  \\
RankNet           & +6.33             & +7.33             & +6               & +6                 & -3.67       & +1.67         & -2.33       & +1.67             & +4.16  \\
Coordinate Ascent & +6.33             & +6                & +8               & +5.33              & -3.67       & +1            & -3          & -0.3              & +3.75  \\
Lambda MART	  & +4		      & +2.33		  & +4		     & +6		  & -7.33	& -1.33		& -5.33	      & -0.3		  & +2.34  \\		
MART	 	  & +5.67	      & +2		  & +6.33	     & +2.33		  & -4.33	& -4		& -6.77	      & -5		  & +0.75 \\			
RankBoost	  & +4.67	      & +3		  & +4		     & +5		  & -5.67	& -3		& -7	      & -4.67		  & +0.67 \\
% Random Forests    & +5.33             & -1.67             & +5.33            & +3.67              & -5          & -1.67         & -5.33       & -2.33             & 1.67  \\


\bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[h!]
\caption{Learning to Rank Percentage of unjudged documents in Top 10.}
\label{l2r-res}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
\toprule
Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule

Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule

Linear Regression & 4              & 5                 & 10               & 12                 & 4              & 1                 & 8                & 7        & 6.25          \\
AdaRank           & 4              & 4                 & 12               & 10                 & 1              & 1                 & 9                & 7        & 6         \\
RankNet           & 9              & 8                 & 17               & 17                 & 9              & 1                 & 11               & 7        & 8.25         \\
Coordinate Ascent & 4              & 7                 & 12               & 16                 & 9              & 5                 & 12               & 8        & 7.25         \\
Lambda MART       & 11             & 9                 & 17               & 19                 & 11             & 4                 & 16               & 11       & 11.25         \\
MART              & 9              & 1                 & 15               & 19                 & 9              & 8                 & 14               & 15       & 11.75       \\
RankBoost         & 11             & 12                & 19               & 20                 & 11             & 8                 & 16               & 13       & 13       \\

\bottomrule
\end{tabular}%
}
\end{table}

% 
% \begin{figure}[h!]
% \centerline{
%   \includegraphics[scale=0.3]{../../ranking/plots/unjudged.png}
%   }
%   \caption{Unjudged docs.}
%   \label{unjudged-fig}
% \end{figure}
% 
% \begin{figure}
% \centerline{
%   \includegraphics[scale=0.27]{../../ranking/plots/bar/regression.regression.png}
%   }
%   \caption{P@10 improvements per query using linear regression for reranking.}
%   \label{regression-query}
% \end{figure}

\subsection{Comparison with prev. paper}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Comparison with prev. paper}
\label{my-label}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
                  & \textbf{Results prev. paper results} & \textbf{Prev. paper method, our results} & \textbf{Our best method} \\\midrule
Summaries 2014    & 33.67\tablefootnote{Without re-ranking diagnosis queries, results is 34.67} (32 + 1.67)                   & 36.33 (31.67 +4.67)                      & 40 (31.67 +8.33)         \\
Descriptions 2014 & 33.00\tablefootnote{Without re-ranking diagnosis queries, results is 36.33} (32 + 1)                      & 32.00 (25 + 7)                           & 35 (25+10)              \\
\bottomrule
\end{tabular}%
}
\end{table}





