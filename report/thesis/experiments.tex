\chapter{Experimental Results}\label{exp}

Let us now present the experimental results of applying our system formally defined in Chapter \ref{meth} and implemented in Chapter 
\ref{impl}
to the TREC 2014 and 2015 Clinical Decision Support track.
We first discuss the evaluation metric and the classifier parameters,
then we move on to discussing the results for the unsupervised fusion methods (Linear Combination, RRF and Borda)
and then the results for the supervised fusion methods (Learning to Rank). We then do an overall comparison between the best results from each strategy
and, finally, we compare our overall best with the results from \cite{choi}.

\section{Query runs}\label{runs} Before we begin, let us recall that we have four \emph{runs} with 30 queries each:
\begin{itemize}
 \item using the query \emph{summaries} from \emph{2014};
 \item using the query \emph{descriptions} from \emph{2014};
 \item using the query \emph{summaries} from \emph{2015};
 \item using the query \emph{descriptions} from \emph{2015};
\end{itemize}

We present a set of results for each of these runs and also three \emph{mean results}:
\begin{itemize}
 \item the mean for 2014 (summaries and descriptions);
 \item the mean for 2015 (summaries and descriptions);
 \item the overall mean over all four runs.
\end{itemize}

\section{Evaluation Metric}
To evaluate the quality of our rankings, we use the precision at 10 (P@10) metric, which counts for each query the percentage of
relevant results from the top 10 retrieved results. For example, if 4 documents of the top 10 retrieved results are relevant,
then P@10 is equal to 40\%. The P@10 is then averaged over all queries in a run.

As discussed in Section \ref{qrels}, the ground truths --- relevance judgements --- are provided by medical experts
and stored in the (\emph{qrels}) files.

\section{Classifier Parameters}
Table \ref{params} lists the 17 classifiers we use and their configurations. 
We empirically determine
the 
parameters by varying them in a cerain range (for example, the regularization parameters
in the $[10^{-4},\cdots,10^4]$ in steps of powers of 10) and choosing the ones
that give the
best P@10 improvement averaged over all four categories.

For the Multilayer Perceptron classifier, we achieved the best results by using two layers of size 50 and 5 respectively
(we ranged the layer sizes from 5 to 100 and the number of layers from 1 to 3).

For the Convolutional Neural Networks classifier, we achieved the best results by
using 100 convolution filters of size 3, 4 and 5, respectively and 50 nodes in last hidden layer; the word-embedding size is 100
(we ranged the number of filters from 50 to 150 and the number of nodes in the last hidden layer from 0, i.e., no hidden layer, to 75).

The weight $\alpha$ of the Keyword Counter score to the overall classifier score (formula \ref{clf-combine-formula}) is set to 0.4 and
the \textsf{cutoff} is set to 4.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Best classifier parameters found}
\label{params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Classifier}             & \textbf{Loss function}  & \textbf{Penalty} & \textbf{Regularization parameter}                             &  &  \\ \midrule
SVM               		& Precision@K		 & L2               & $10^{-3}$                                                 &  &  \\
SVM               		& Recall @K		 & L2               & $10^{-3}$                                               &  &  \\
SVM           			& ROC area		 & L2               & $10^{-3}$                                                &  &  \\
SVM           			& Error-rate		& L2               & $1$                                                     &  &  \\
SVM                 		& F1			& L2               & $1$                                                     &  &  \\
SVM           			& Zero/one		& L2               & $10^{-2}$                                                  &  &  \\
SVM      			& $\epsilon$-insensitive	& Elasticnet       & $10^{-2}$                                              &  &  \\
SVM              		& Hinge 		& L2               & $10^{-3}$                                                 &  &  \\
SVM       			& Squared hinge		& L2               & $10^{-3}$                                                 &  &  \\
SVM             		& Squared		& Elasticnet       & $10^{-2}$                                              &  &  \\
Ridge               		& Squared		 & L2               & $10^{-4}$                                             &  &  \\
Logistic Regression             & Log			& L2               & $10^{-1}$                                                   &  &  \\
Passive Aggressive		& Hinge 		&             & $10^{-2}$                                                  &  &  \\
Perceptron                      &			& Elasticnet             & $10^{-4}$                                            &  &  \\
Multilayer Perceptron          & Log			& Elasticnet       & $10^{-6}$                                                        &  &  \\
Neural Networks                 & Cross-entropy		& L2               & $10^{-4}$ &  &  \\ 
Naive Bayes                    	& 			 &              & $10^{-3}$ (Laplace smoothing)                            &  &  \\\bottomrule
\end{tabular}%
}
\end{table}

\section{Unsupervised Fusion Results}
Let us now present the results for the unsupervised fusion methods. 

For each of the four runs listed in Section \ref{runs} and for each of the seventeen classifiers listed in Table \ref{params}, we perform a fusion of scores
between the baseline BM25 similarity scores and the classifier scores. 
We therefore produce seventeen sets of results for each of the four runs,
one set of results per classifier.
We then average for each classifier the results of the corresponding four runs, producing seventeen \emph{average results}.
This shows which classifier leads to the highest P@10 improvement for that fusion method.

Furthermore, we look at the contribution of the keyword counter towards the overall P@10 improvement. More precisely,
we look at what the improvement is \emph{with} (default) versus \emph{without} using the keyword counter, that is, using $\alpha=0.4$
versus using $\alpha=0$, respectively, in formula \ref{clf-combine-formula}.

Lastly, using the best classifier, we show for each run the improvement for the 30 queries individually.

\subsection{Linear Combination Results}
We begin by showing the results of the Linear Combination fusion method (formula \ref{interp-formula}) in table \ref{interpolation-res}.
The weight $\lambda$ is set to $0.7\pm 0.02$.

The first line shows the \emph{baseline} P@10 for each of the four runs.
The next lines show the \emph{absolute} P@10 \emph{improvements} for each classifier.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Linear Combination} --- absolute P@10 improvements.}
\label{interpolation-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\\midrule
Baseline                             & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
SVM (with Precision@K loss)                                 & +8                       & +9                          & +\textbf{4}              & +3.33                       & +\textbf{6.08}    \\ 
SVM (with Rec@K loss)                                    & +\textbf{8.33}           & +\textbf{10}                & +3                       & +2.67                       & +6                \\ 
Ridge                                          & +7                       & +9                          & +3.33                    & +\textbf{4}                 & +5.83             \\ 
SVM (with ROC area loss)                                    & +7.67                    & +9.67                       & +2                       & +3.67                       & +5.75             \\ 
SVM (with Error-rate loss)                                 & +8                       & +8.67                       & +3                       & +3                          & +5.67             \\ 
SVM (with Squared hinge loss)                           & +7                       & +9                          & +3                       & +3.67                       & +5.67             \\ 
Logistic Regression                            & +7.67                    & +8.67                       & +3                       & +3                          & +5.59             \\ 
SVM (with Squared loss)                               & +7.67                    & +9.33                       & +3.33                    & +1.67                       & +5.5              \\ 
SVM (with F1 loss)                                       & +7                       & +8.67                       & +2.67                    & +3.33                       & +5.42             \\ 
SVM (with Zero/one loss)                                        & +7                       & +8.67                       & +3                       & +3                          & +5.42             \\ 
Passive Aggressive                             & +6                       & +8                          & +2.33                    & +4                          & +5.08             \\ 
SVM (with $\epsilon$-insensitive loss)                            & +6.33                    & +7.67                       & +2.67                    & +3                          & +4.92             \\ 
SVM (with Hinge loss)                                     & +5.33                    & +8                          & +2                       & +3.67                       & +4.75             \\ 
Multilayer Perceptron                           & +7.67                    & +8                          & +1.67                    & +1                          & +4.59             \\ 
Neural Networks                               & +3                       & +5.67                       & +2.33                    & 0                          & +2.75             \\ 
Perceptron                                      & +4.33                    & +4.33                       & +0.33                    & 0                          & +2.25             \\ 
Naive Bayes                                      & +3                       & +4                          & +1                       & -1                         & +1.75             \\ \bottomrule
\end{tabular}%
}
\end{table}

The best rerankers are the ones using SVM classifiers, with the top one giving an overall improvement of over 6 percentage
points in P@10. For 2014, the best classifier was SVM with Rec@K loss (8.33\% and 10\% improvements), 
and for 2015 they were SVM with Precision@K loss and Ridge (4\% improvements).

In particular, for summaries 2014, the top reranked result is $31.67\%+8.33\% = 40\%$, which is better than the top automatic submission
from TREC 2014 (39\%).

Generally, the improvements for 2014 are at least 
twice as high as the ones for 2015, but this may be due to the lower baseline precisions from 2014 compared to 2015.

Looking more closely, we can see that the top 10 classifiers are not more than 0.7\% worse off compared to the best,
which means that any one of them could be trusted to work well with other queries.

However, the bottom three classifiers are more modest, yielding improvements less than 3\%.
In the case of neural networks, the problem might be the small size of the training data,
while the Perceptron and Bayes classifiers are too naive for this task.
Nevertheless, they still give some moderate improvements for 2014 and Summaries 2015.

\subsection{Reciprocal Rank Fusion Results}
Let us now show the results of the Reciprocal Rank fusion method (formula \ref{rrf-formula}) in table \ref{rrf-res}.
The weight $\lambda$ is set to $0.6\pm 0.02$ and $k$ is set to 60.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Reciprocal Rank Fusion} --- absolute P@10 improvements.}
\label{rrf-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
                                    & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\ \midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule

SVM (with Squared loss)                               & +5                       & +7.33                       & +\textbf{3.67}                    & +\textbf{3}                          & +\textbf{4.75}             \\ 
SVM (with Rec@K loss)                                                                      & +5                       & +\textbf{7.67}                       & +{3}              & +2                          & +{4.42}    \\ 
Passive Aggressive                                       & +\textbf{5.33}           & +5.67              & +2.33                    & +2.67                       & +4                \\ 
SVM (with Precision@K loss)                                                                 & +3.67                    & +7                          & +3.33                    & +{1.67}              & +3.92             \\ 
SVM (with F1 loss)                                                                          & +4.33                    & +6.33                       & +2.67                    & +2.33                       & +3.92             \\ 
Logistic Regression                                                      & 3.67                    & +7                          & +2.67                    & +2                          & +3.84             \\ 
SVM (with Hinge loss)                                                                        & +4                       & +5.67                       & +2.67                    & +\textbf{3}                          & +3.84             \\ 
SVM (with Error-rate loss)                                                                    & +4.33                    & +6                          & +3.33                    & +1.67                       & +3.83             \\ 
SVM (with Squared hinge loss)                                                              & +3.67                    & +6.33                       & +2.33                    & +2.67                       & +3.75             \\ 
Ridge                                                             & +3.33                    & +7                          & +2.33                    & +2                          & +3.67             \\ 
SVM (with $\epsilon$-insensitive loss)                         & +4.33                    & +6.67                       & +1.67                    & +1.67                       & +3.59             \\ 
SVM (with ROC area loss)                                                                    & +3.33                    & +7                          & +2.33                    & +1.67                       & +3.58             \\ 
SVM (with Zero/one loss)                                                                        & +4                       & +5.67                       & +2.33                    & +2.33                       & +3.58             \\ 
Multilayer Perceptron								& +4 & +6.33 & +2 & +0 & +3.08 \\
Naive Bayes                                                                      & +5                       & +5.67                       & +1.33                    & -1                         & +2.75             \\ 
Neural Networks                                                                   & +3.67                    & +4.67                       & 0                       & +1                          & +2.34             \\ 
Perceptron                                                              & +2                       & +3                          & +1                       & 0                          & +1.5              \\ \bottomrule
\end{tabular}%
}
\end{table}

Similar to the linear combination method, SVM classifiers generally work best,
although the top one now uses the squared loss instead of Precision@K loss (which takes fourth place instead).

RRF, however, gives lower improvements, with the highest one being 4.75\%
compared to 6\% for linear combination.
This may be due to the fact that RRF loses some information by only using the ranks and not the actual scores. On the other hand, RRF
is easier and more convenient to compute, since no score scaling is needed.


\subsection{Borda Fusion Results}
We show the results of the Borda fusion method (formula \ref{borda-formula}) in table \ref{borda-res}.
The weight $\lambda$ is set to $0.6\pm 0.05$.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Weighted \textbf{Borda Fusion} --- absolute P@10 improvements.}
\label{borda-res}
\resizebox{1.15\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
                                       & Summaries 2014 & Descriptions 2014 & Summaries 2015 & Descriptions 2015 & Average       \\\midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
                                   
SVM (with Squared loss)                               & +\textbf{4.67}  & +\textbf{8.67}     & +2.33           & +\textbf{1.33}     & +\textbf{4.25} \\
SVM (with Precision@K loss)                            & +4              & +9                 & +1.67           & +\textbf{1.33}     & +4             \\
SVM (with Rec@K loss)                                 & +4.33           & +7.67              & +3.33           & +0.67              & +4             \\
Logistic Regression                                    & +4.33           & +8                 & +2              & +\textbf{1.33}     & +3.92          \\
SVM (with Error-rate loss)                             & +4.33           & +6                 & +3.33           & +1                 & +3.67          \\
SVM (with F1 loss)                                     & +4              & +5.67              & +4              & +1                 & +3.67          \\
SVM (with Squared hinge loss)                           & +3              & +8                 & +2              & +\textbf{1.33}     & +3.58          \\
SVM (with Zero/one loss)                               & +3.67           & +5.67              & +\textbf{3.67}  & +1                 & +3.5           \\
Ridge                                                   & +3              & +8                 & +1.33           & +1                 & +3.33          \\
SVM (with ROC area loss)                                & +3.67           & +8                 & +1.33           & +0.33              & +3.33          \\
SVM (with $\epsilon$-insensitive loss)                   & +3.33           & +7                 & +1.33           & +1                 & +3.17          \\
Passive Aggressive                                       & +3.67           & +5                 & +1.67           & +1                 & +2.84          \\
SVM (with Hinge loss)                                    & +2              & +6.33              & +0.67           & +1                 & +2.5           \\
Multilayer Perceptron                                 & +2.67           & +6.33              & +1              & -1                & +2.25          \\
Naive Bayes                                              & +3.33           & +6.33              & +0.33           & -3                & +1.75          \\
Neural Networks                                         & +1              & +4.33              & -1             & -1                & +0.83          \\
Perceptron                                              & +1.33           & +2                 & +0.67           & +-2.33             & +0.42         \\\bottomrule
\end{tabular}%
}
\end{table}

Again, SVM classifiers work best, the top one using the squared loss, like RRF. 
However, the results for Borda are generally lower than for RRF by around 0.5\%. This is mostly due
to poorer results for 2015 (1.83\% vs. 3.33\% for the top result).
On the other hand, Borda works slightly better for 2014 (6.67\% vs 6.16\% for the top result).

Overall, the results of the three fusion methods indicate that \[\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}.\]

\subsection{Relation between weight $\lambda$ and improvement}
Let us now investigate the relation between the weight $\lambda$ and the improvement in P@10 for each of the three
fusion methods, when using their best classifier (i.e., SVM with Precision@K loss for linear combination
and SVM with squared loss for RRF and Borda).
Recall from formulas \ref{interp-formula}, \ref{rrf-formula} and \ref{borda-formula} that the weight $\lambda$ is the 
weight of the baseline BM25 score and $1-\lambda$ is the weight of the classifier score.

Figure \ref{lambda-vary} shows for each of the three methods
how the improvement in P@10 behaves as the weight $\lambda$ is varied between 0 and 1 in steps of 0.01.

\begin{figure}[h!]

\begin{subfigure}[b]{\textwidth}
 \centerline {
  \includegraphics[scale=0.22]{../../ranking/plots/interpolation/all/all.SVMPerf.04.0.001.hedges.png}
  }
%   \vspace{-0.1cm}
%   \caption{P@10 improvements using linear combination and SVM with Precision@K loss function.}
  \label{interp-weight}
\end{subfigure}
%  \vspace{0.5cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/rrf/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using RRF and SVM with squared loss function.}
  \label{rrf-weight}
\end{subfigure}
% \vspace{0.2cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/borda/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using Borda fusion and SVM with squared loss function.}
  \label{borda-weight}
\end{subfigure}
 
 \caption{Relation between weight $\lambda$ and improvement}
 \label{lambda-vary}
 
\end{figure}

The first thing we see in Figure \ref{lambda-vary} is that the improvements generally happen for $\lambda \in [0.5, 0.8]$.
As also reflected in tables \ref{interpolation-res}, \ref{rrf-res} and \ref{borda-res}, the 2014 improvements are much higher than
the 2015 ones. 

\paragraph{Observation} Intrestingly, for Descriptions 2014, using $\lambda = 0$ gives better results
than using $\lambda=1$. This does not mean, however, that for 2014, using the classifier scores 
works better than using the BM25 scores, but rather that
\emph{re-sorting} the top 100 BM25 results by their classifier scores is better. 
We still need the BM25 retrieval step to work as a ``filter'',
since otherwise the top retrieved documents would be the ones with the highest classifier scores out of the whole document collection, 
regardless of whether
they have any \emph{textual} similarity to the query --- this is also discussed in Chapter \ref{meth}.
In any case,  however, this peculiarity only happens for 2014; for 2015 the improvements happen only when $\lambda>0.5$,
and even for 2014, the highest improvements happen also when $\lambda>0.5$.

Moving forward with the analysis of Figure \ref{lambda-vary}, we see that
the improvements for 2014 and 2015 are more correlated with each other when using linear combination than when using
RRF or Borda. By this we mean that the two weights that give the peak average improvements for 2014 and 2015 are closer to each other
when using linear combination (both around $0.7$) than they are when using RRF or Borda ($0.4$ vs $0.6$ for 2014 and 2015, respectively).
This ``mismatch'' might also contribute to the lower average peak precision for RRF and Borda ($<4.8\%$) 
vs. Linear Combination ($6\%$), despite the fact that they sometimes 
give higher peak precisions for 2014 (e.g. peak Descriptions 2014 improvement 
is $\sim10\%$ for Borda vs. only $\sim9\%$ for Linear combination). However, the 2015 improvements are always higher for
Linear Combination.

Looking beyond these granular differences and more at the overall picture, we can safely conclude
that choosing any of the three unsupervised fusion methods and any weight $\lambda$ between 0.6 and 0.8 would give
a solid improvement over the baseline.

\subsection{Contribution of the Keyword Counter}
Recall from formula \ref{clf-combine-formula} that the classifier score is actually 
a linear combination of the machine learning classifier score (e.g., SVM) and the keyword counter score. The idea, 
explained in Section \ref{intent-scoring}, 
was that they keyword counter would double-check the classifier and maybe correct its predictions by
increasing or lowering the scores depending on the frequency of certain keywords.

Figure \ref{basic-effect-plot}
confirms the fact that the keyword counter helps. The average improvement for linear combination is more than 1\% higher
when using it than when not using it and the $\lambda$--P@10 curve is generally wider.

For RRF and Borda the effect is less pronounced --- probably because they only use the ranks and not the scores ---
but is still there nonetheless.

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/basic-vs-non-basic.png}
  }
  \caption{Contribution of using the Keyword Counter.}
  \label{basic-effect-plot}
\end{figure}

\subsection{Improvement per query and intent type}
We conclude our analysis of the unsupervised fusion methods by examining the improvement per query and intent type.

Figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query} show the change (increase or decrease) in P@10 for each query,
using Linear Combination, RRF and Borda fusion, respectively, using the best classifier for each method (SVM with
Precision@K and squared losses, respectively) and for each of the four runs, \{Summaries, Descriptions\} $\times$ \{2014, 2015\}.

We summarize the findings in Table \ref{change-per-query-table}, which counts
how many queries got better or worse after the reranking.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Number of better/worse queries.}
\label{change-per-query-table}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
			& \textbf{Better 2014} & \textbf{Worse 2014} & \textbf{Better 2015} & \textbf{Worse 2015} \\ \midrule
Linear Combination 	& 34                   & 4                   & 24                   & 10                  \\
Reciprocal Rank Fusion  & 27                   & 3                   & 22                   & 10                  \\
Borda fusion       	& 30                   & 9                   & 24                   & 20                  \\ \bottomrule
\end{tabular}%
}
\end{table}

This reinforces the idea that $\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}$
and that 2014 is easier to rerank than 2015.

Looking more closely at figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query} 
it is clear that
the treatment queries (21-30) fared \emph{much} better than the diagnosis queries (1-10) 
which fared \emph{slightly} better than the test queries (11-20).
This may be due to having only about 1000 positive samples for training the diagnosis classifier vs. 8000 positive samples
for training the treatment classifier. Moreover, we did not have any training samples for the test classifier,
so we had to use the diagnosis samples instead.

We summarize these findings in Table \ref{change-per-query-table-lincomb},
where we list the improvements in P@10 for each intent type using the Linear Combination method with SVM--Precision@K
classifier.

% Table \ref{change-per-query-table-lincomb} lists this distribution of query changes per intent
% type for Linear Combination.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Linear Combination - Change in P@10 per intent type.}
\label{change-per-query-table-lincomb}
\resizebox{\textwidth}{!}{%

\begin{tabular}{@{}llllll@{}}
\toprule
          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\\midrule
Diagnosis & +4                      & +11                        & +2                      & +3                         & \textbf{+5}               \\
Test      & +10                     & +4                         & +2                      & +3                         & \textbf{+4.75}            \\
Treatment & +10                     & +12                        & +8                      & +4                         & \textbf{+8.5}             \\\midrule
Average   & +8                      & +9                         & +4                      & +3.33                      & \textbf{+6.08}           \\\bottomrule
\end{tabular}%

}
\end{table}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SVMPerf.04.0.001.hedges.interpolation.png}
  }
  \caption{Linear Combination - P@10 change per query.}
  \label{interp-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.rrf.png}
  }
  \caption{RRF - P@10 change per query.}
  \label{rrf-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.borda.png}
  }
  \caption{Borda Fusion - P@10 change per query.}
  \label{borda-query}
\end{figure}


\section{Supervised Fusion Results}
Let us now move on to the supervised learning results. As mentioned in Sections \ref{sup-fusion} and \ref{impl-sup}, we use several
learning to rank algorithms which we train on the queries from one year and test on the queries from another year.

The features that we use are the baseline BM25 similarity scores and the classifier scores, computed by the SVM 
classifier with Precision@K loss function --- the best one according to the unsupervised linear combination method.

Since the learning to rank algorithms have several parameters, we performed a search for the ones that give the best
results (for example we varied the number of iterations or the number of trees from 5 to 500 in steps of 10).
The best parameters found are listed in Table \ref{l2r-params}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Learning to Rank Parameters}
\label{l2r-params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|l@{}}
\toprule
Algorithm         & \textbf{Parameters}                                                             \\\midrule
RankNet           & 40 iterations, 1 hidden layer, 10 nodes/layer                                   \\
RankBoost         & 40 iterations                                                                   \\
AdaRank           & 10 iterations, max number of times a feauture can be consecutively selected: 2  \\
Coordinate Ascent      & 20 iterations, Regularization parameter: $10^{-3}$                  \\
MART              & 100 trees, 10 leaves                                                            \\
Lambda MART       & 60 trees, 10 leaves                                                             \\
Linear Regression & Regularization parameter: $10^{-10}$      				\\\bottomrule                        
\end{tabular}%
}
\end{table}

Table \ref{l2r-res} lists the final results, with the first and second lines 
specifying the training and test sets used. For example, for test queries \emph{Summaries 2014},
either the \emph{Summaries 2015} or the \emph{Descriptions 2015} training sets can be used. 
We list the results for both training sets as two columns.

The \emph{Best Average} column shows the \emph{best} average of the four test query runs.
That is, for each run, we use the training set (either \emph{Summaries} or \emph{Descriptions}) that gives the highest
improvement. For example, the best average for \emph{Linear Regression} uses the \emph{Summaries 2015} training set for the
\emph{Summaries 2014} and \emph{Descriptions 2014} test set, 
the \emph{Summaries 2014} training set for the \emph{Summaries 2015} test set, and 
the \emph{Descriptions 2014} training set for the \emph{Descriptions 2015} test set.

For convenience, we \textit{italicize} the results contributing to the Best Average column.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\caption{Supervised fusion (learning to rank) results.}
\label{l2r-res}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
\toprule
Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule

Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule
Baseline          & \multicolumn{2}{c|}{31.67}          & \multicolumn{2}{c|}{25}              & \multicolumn{2}{c|}{38.67}     & \multicolumn{2}{c|}{35}     &32.59         \\ \midrule

Linear Regression & \textit{\textbf{+7}} & +6.67             & \textit{\textbf{+8.33}}   & +7.67              	& \textit{+2.67}       	& +1.67    		& +1          & \textit{\textbf{+2.33}}   & +\textbf{5.08}  \\
AdaRank           & \textit{+6.33}       & +3.33             & \textit{+8}               & +6.67              	& \textit{\textbf{+3}}	& +1.67         	& +1.33       & \textit{\textbf{+2.33}}   & +4.92  \\
RankNet           & +6.33             	 & \textit{+6.67}    & \textit{+6}               & +6                 	& -3.67       		& \textit{+1.67}        & -2.33       & \textit{+1.67}            & +4  \\
Coordinate Ascent & \textit{+6.33}       & +6                & \textit{+8}               & +5.33              	& -3.67      		& \textit{+1}           & -3          & \textit{-0.3}             & +3.75  \\
Lambda MART	  & \textit{+4}		 & +2.33	     & +4		         & \textit{+6}	   	& -7.33			& \textit{-1.33}	& -5.33	      & \textit{-0.3}		  & +2.09  \\		
MART	 	  & \textit{+5.67}	 & +2		     & \textit{+6.33}	         & +2.33		& -4.33			& \textit{-4}		& -6.77	      & \textit{-5}		  & +0.75 \\			
RankBoost	  & \textit{+3.67}	 & +3		     & +4		         & \textit{+5}		& -5.67			& \textit{-3}		& -7	      & \textit{-4.67}		  & +0.25 \\
% % Random Forests    & +5.33             & -1.67             & +5.33            & +3.67              & -5          & -1.67         & -5.33       & -2.33             & 1.67  \\

\bottomrule
\end{tabular}%
}
\end{table}

Let us now analyze the results. We can definitely say that Linear Regression and AdaRank (around \%5 improvement)
outperform the other five methods. Linear Regression is the best for 2014 overall (+7.67\%) and 
AdaRank is the best for 2015 overall (+2.67\%).

The next two methods, RankNet and Coordinate Ascent, give similarly good
improvements for 2014 (+6.33\% and +7.17\% respectively), but their performance is somewhat lower
for 2015 (only +1.67\% and +0.7\% respectively).

The bottom three methods, Lambda MART, MART and RankBoost, do not even give an improvement for 2015, being worse
than the baselines by around 0.8\%, 4.5\% and 3.8\%, respectively. However, their 2014 improvements are enough to compensate
for this, so they all have positive overall improvemenets.

One potential reason for the lower performance of the latter three methods might be the fact that they generate high
numbers of unjudged documents in the top 10. Even though some of these documents \emph{might} be relevant,
they are by default considered irrelevant.
Figure \ref{unjudged-fig} shows for each learning to rank algorithm
the relation between the average P@10 improvments and the average 
percentage of unjudged documents in Top 10. The top four methods
have substantially lower percentages of unjudged documents (below 8.25\%) than the bottom
three methods (above 11.25\%), which \emph{might} explain \emph{some} of performance degradation.
Of course, we cannot
be sure if this is indeed the cause, or if simply these methods are less suited for the task at hand.

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.3]{../../ranking/plots/unjudged.png}
  }
  \caption{P@10 improvements vs. unjudged documents in Top 10.}
  \label{unjudged-fig}
\end{figure}

Let us now decide which training sets were better.
Looking at Table \ref{l2r-res}, we see that for both Summaries 2014 and Descriptions 2015, the best
training set was \emph{generally} Summaries 2015. 
For Summaries 2015, the best training set 
was Summaries 2014 for the top two methods and Descriptions 2014 for the other five,
while for Descriptions 2015, the best training set was \emph{always} Descriptions 2014.
For the top two methods, therefore, summaries made better training sets than descriptions in all
four cases except Descriptions 2015.

Let us conclude the analysis of the supervised fusion results with a look at the P@10 improvement per query
and intent type
for the best method --- Linear Regression. Figure \ref{regression-query} and Table 
\ref{change-per-query-table-regression} show the change in P@10
per query and per intent type, respectively, for each of the four runs.

We can see that, similar to the unsupervised fusion methods,
Treatment queries fared better than Diagnosis and Test queries, especially for Summaries 2014 and 2015.
As discussed for the unsupervised fusion methods, this may be due to the much lower number of training
documents for Diagnosis and Test (1000 positive samples) vs Treatment (8000 positive samples).
In addition, 2014 seems much easier to improve than 2015, especially the Diagnosis and Test queries.

\begin{table}[h!]
\centering
\caption{Linear Regression - Change in P@10 per intent type.}
\label{change-per-query-table-regression}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\ \midrule
Diagnosis & +5                      & +11                        & +1                      & +1                         & \textbf{+4.5}             \\
Test      & +7                      & +3                         & 0                       & +3                         & \textbf{+3.25}            \\
Treatment & +9                      & +11                        & +7                      & +3                         & \textbf{+7.5}             \\ \midrule
Average   & +7		 	    & +8.33			 & +2.67		   & +2.33			& \textbf{+5.08}	 \\			

\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/regression.regression.png}
  }
  \caption{Linear Regression - P@10 change per query.}
  \label{regression-query}
\end{figure}

\section{Overall comparison between fusion methods}
Let us now finally do an overall comparison between the best fusion methods: 
the three unsupervised ones (Linear Combination, RRF and Borda)
and the top three supervised ones (Linear Regression, AdaRank and RankNet).
Since for Linear Combination both SVM classifiers with Precision@K and Recall@K
losses give improvements of at least $6\%$, we list both of them in the comparison
(Table \ref{fusion-comp}).

\begin{table}[h!]
\centering
\caption{Comparison between best fusion methods.}
\label{fusion-comp}
\resizebox{\textwidth}{!}{%

\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} 	\\\midrule
Baseline                 	& 31.67     	& 25  		& 38.67          	& 35        	&   32.59      		\\ \midrule
Lin Comb - Prec@K		& +8          	& +9     	& \textbf{+4}		& \textbf{+3.33}& \textbf{+6.08}    	\\ 
Lin Comb - Rec@K		& \textbf{+8.33}& \textbf{+10}	& +3			& +2.67		& \textbf{+6}		\\
Lin Regression			& +7		& +8.33		& +2.67		   	& +2.33		& \textbf{+5.08}	 \\			
AdaRank				& +6.33		& +8		& +3		   	& +2.33		& \textbf{+4.92}	 \\			
RRF                             & +5         	& +7.33    	& +3.67            	& +3  		& \textbf{+4.75}        \\ 
Borda				& +4.67  	& +8.67     	& +2.33           	& +1.33    	& +\textbf{4.25} 	\\
RankNet				& +6.67		& +6		& +1.67			& +1.67		& \textbf{+4}		\\
\bottomrule
\end{tabular}
}
\end{table}

The best method overall is therefore Linear Combination using an SVM classifier. For 2014, the best loss function
is Recall@K, while for 2015 the best loss function is Precision@K.

Compared to the unsupervised fusion methods, both Linear Regression and AdaRank sit somewhere in between
Linear Combination (+6\%) and RRF (+4.75\%). Of course, one advantage of the learning to rank algorithms
is that we do not need to manually search for the best weight $\lambda$ between the two features, 
since it is automatically determined
from the training data. 
We do need, however, to search for the best hyper-parameters, such as number of iterations, regularization
parameters and so on. Thus, there are tradeoffs for each approach (supervised vs. unsupervised), 
but nevertheless they each give solid average improvements of at least 4-5\% over the baseline.

% \begin{table}[h!]
% \caption{Learning to Rank Percentage of unjudged documents in Top 10.}
% \label{l2r-res}
% \resizebox{1.2\textwidth}{!}{%
% \begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
% \toprule
% Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule
% 
% Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule
% 
% Linear Regression & 4              & 5                 & 10               & 12                 & 4              & 1                 & 8                & 7        & 6.25          \\
% AdaRank           & 4              & 4                 & 12               & 10                 & 1              & 1                 & 9                & 7        & 6         \\
% RankNet           & 9              & 8                 & 17               & 17                 & 9              & 1                 & 11               & 7        & 8.25         \\
% Coordinate Ascent & 4              & 7                 & 12               & 16                 & 9              & 5                 & 12               & 8        & 7.25         \\
% Lambda MART       & 11             & 9                 & 17               & 19                 & 11             & 4                 & 16               & 11       & 11.25         \\
% MART              & 9              & 1                 & 15               & 19                 & 9              & 8                 & 14               & 15       & 11.75       \\
% RankBoost         & 11             & 12                & 19               & 20                 & 11             & 8                 & 16               & 13       & 13       \\
% 
% \bottomrule
% \end{tabular}%
% }
% \end{table}

\section{Comparison with previous research}
We conclude this section by doing a comparison with previous research in the TREC CDS area. 
As discussed in Section \ref{related}, the best previous reseach using the query intent type for reranking
was done by \cite{choi}. However, they did not write a paper for 2015, which means we can only compare the 2014 results.

Recall that \cite{choi} uses a similar idea like us, namely fusing baseline scores and
classifier scores. They also use the Clinical Hedges Database for training and SVM for classification.
However their SVM uses ROC-area loss and their fusion uses Borda with a weight $\lambda=0.5$.
Similar to us, they also use the diagnosis samples for the test classifier.
The runs that they re-rank are the \emph{ExternalQE} runs, where 
they also use externally-tagged knowledge-based query expansion in addition to the baseline similarity score
(runs \emph{Baseline}).

Let us now list the runs that we will compare:
\begin{itemize}
 \item from \cite{choi}, runs \emph{SNUMedinfo1} and \emph{SNUMedinfo4} for Summaries 2014 and Descriptions 2014, respectively.
  In these runs --- like in our system --- all queries are reranked;
 \item from \cite{choi}, runs \emph{SNUMedinfo3} and \emph{SNUMedinfo6} where the Diagnosis queries are \emph{not} re-ranked;
 \item our results using the approach from \cite{choi} --- Borda Fusion, SVM classifier with ROC-Area loss and $\lambda=0.5$;
 \item our results using the best unsupervised fusion --- Linear Combination, SVM classifier with Recall@K loss and $\lambda=0.7$;
 \item our results using the best supervised fusion --- Linear Regression trained on Summaries from 2015.
\end{itemize}

Table \ref{choi-comp} summarizes the results of the comparison.
Before delving into the discussion, let us mention that since we and \cite{choi} have different baselines, 
we cannot do a real, unequivocal comparison. Different baselines means different potentials for improvement, and
not even having the same P@10 is enough, as exemplified by the difference in improvements for Summaries
2014 vs. Descriptions 2014 for \cite{choi} (both baselines are 32\%, but the improvements are different).
We nevertheless do the comparison in order to see if we can recommend other approaches that \emph{seems}
to perform better than the ones in \cite{choi}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Comparison with previous research --- 2014}
\label{choi-comp}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
                          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} \\\midrule
Baseline from \cite{choi}     			& 32     			& 32                         \\\midrule
Improvements from \cite{choi} 
- diagnosis reranked  				& +1.67  			& +1                          \\
Improvements from \cite{choi}
- diagnosis not reranked 			& +2.67   			& +4.67                       \\\midrule
%                           &                         &                            \\
Our baseline             			& 31.67                   	& 25                         \\\midrule
Our improvements - method from \cite{choi} 	& +4.67                    	& +7                          \\
Our improvements - best unsupervised fusion 	& +8.33                    	& +10                         \\
Our improvements - best supervised fusion     	& +7                       	& +8.33                      \\\bottomrule
\end{tabular}%
}
\end{table}

First, let us observe that the our and \cite{choi}'s Summaries baselines have almost equal P@10 (32\% and 31.67\%),
but the Descriptions baselines are very different (ours is 25\% and \cite{choi}'s is 32\%).
This might be due to the fact that they also did externally-tagged knowledge-based query expansion
and possibly different data pre-processing for the
run that we consider as baseline.

Second, for \cite{choi}, re-ranking the Diagnosis queries is worse than \emph{not} re-ranking them, which means that
their system did not work well for the Diagnosis queries and made them worse.
Our system, however, improves all three types of queries 
(see Tables \ref{change-per-query-table-lincomb} and \ref{change-per-query-table-regression}),
even though the Diagnosis and Test improvements are indeed lower than the Treatment ones.

Third, the absolute improvements are higher when using our methods. 
While the maximum improvement
in \cite{choi} is +2.67\% for Summaries and +4.67\% for Descriptions (both in the runs not reranking Diagnosis queries), 
our improvements are consistently
higher than that --- at least +7\% for Summaries and at least +8.33\% for Descriptions. 

As a result, our best P@10s for Summaries are 40\% and 38.67\%
(unsupervised and supervised, respectively) compared to the best Summaries from \cite{choi}, namely 34.67\%.

For Descriptions, it is more difficult to compare, since our baseline has a much lower P@10. 
Still, our reranked P@10s for Descriptions are 35\% and 33.33\% (unsupervised and supervised, respectively)
 --- better than \cite{choi}'s P@10 of 33\% when reranking all three categories. 
Our Descriptions P@10s are, however, slightly worse
than \cite{choi}'s P@10 of 36.67\% when reranking only the Test and Treatment queries, but keep in mind that
1) we started with a baseline 7\% worse than theirs --- the absolute improvements themselves being much higher, +10\% and +8.33\%
vs +4.67\%, and 2) we have 13\% percent unjudged documents in the top 10 using Linear Combination,
some of which \emph{may} potentially be relevant.

To sum up all of the above, we can safely recommend our best supervised and unsupervised methods
(Linear Regression and Linear Combination) as well as the next-best ones (AdaRank and RRF)
as solid alternatives to previous approaches to the TREC Clinical Decision Support track.






