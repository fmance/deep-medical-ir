\chapter{Experiments}

In this chapter, we present the application of the theory from the previous section. Concretely, we apply the system on Trec 2014 and 2015
clinical decision support track. 
The CDS track consists of medical question, describing symptoms and other relevant information. The question is to diagnose/test/treat
the patient.
The input consists of 30 queries from 2014 and 2015 each. Each query contains description and summary. Also type.
We have approximately 733K documents to search from.

Example query:

We also have ground truths, i.e. labels for around x percent of the query-doc pairs, based on trec 14,15. 

We first one to apply the baseline IR system on the queries, then use the classifier and fusion as described in the prev. section.
This is applied for both 2014, 2015.

\section{Implementation}

\subsection{Computing the similarity scores}
As mentioned in the methodology section, we use BM25 to compute the similarity scores between queries and documents.
We use the Lucene information retrieval system for indexing and searching.

Indexing happens in two stages:
\begin{enumerate}
 \item parsing the NXML file format into plain-text (article title, abstract and body), using Java's DOM parser;
 \item indexing the parsed text, using the \texttt{BM25Similarity} and the \texttt{EnglishAnalyzer} for tokenizing and stemming the text.
\end{enumerate}

At query time, we parse the queries' XML file and retrieve the top 100 results per query, using the same configuration as for indexing.
For each year, 2014 and 2015, we produce two results file: one using the query summaries and another one using the query descriptions.


\subsection{Computing the classifier scores}


\subsection{Fusing the similarity and classifier scores}
Now that we have both the similarity scores and classifier scores, we can finally fuse them together. 
For the unsupervised fusion methods (linear interpolation, RRF and Borda Fuse), we min-max-normalize both the BM25 scores
and the classification scores for each query, and fuse the scores as described in the methodology section. The $\lambda$-weight
is varied in the $[0,1]$ interval, in steps of $0.01$.


\section{Evaluation Metric}
We use precision at 10 (P@10) as the evauation metric. P@10 counts how many relevant results we have in the top 10 retrieved results, for each query.
The relevance judjements for each query are stored in a \emph{qrels} file, with each document in this file receiving a score of 0, 1 or 2, depending on whether it
is not relevant, possibly relevant or definitely relevant, respectively. The relevance judgements were assigned by medical experts after each edition of TREC.

For measuring the P@10, we count both 1 and 2 as relevant. Note that only a
fraction of the 730K documents are actually judged (and in the qrels files), so it may happen that some documents retrieved by our system
are mis-counted as being not relevant simply because they have not been judged.


\section{Results}
\subsection{Classifier Parameters}
Table \ref{params} lists the classifiers we use and their configurations (loss function, penalty). We empirically determine
the regularization
parameters by varying them in a cerain range (for example, $[10^{-4},\cdots,10^4]$ in steps of powers of 10) and choosing the ones
that give the
best P@10 improvement averaged over all four categories.
For the Multilayer-perceptron we use two layers of size 50 and 5 respectively. For the Neural Networks,
we use 100 convolution filters of size 3, 4 and 5, respectively and 50 nodes in next hidden layer. The word-embedding size is 100 
and is computed using \texttt{word2vec} over the preprocessed TREC 2014 document corpus.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{Best classifier parameters found}
\label{params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Classifier}             & \textbf{Loss function}  & \textbf{Penalty} & \textbf{Regularization parameter}                             &  &  \\ \midrule
SVM               		& Precision@K		 & L2               & $10^{-3}$                                                 &  &  \\
SVM               		& Recall @K		 & L2               & $10^{-3}$                                               &  &  \\
SVM           			& ROC area		 & L2               & $10^{-3}$                                                &  &  \\
SVM           			& Error-rate		& L2               & $1$                                                     &  &  \\
SVM                 		& F1			& L2               & $1$                                                     &  &  \\
SVM           			& Zero/one		& L2               & $10^{-2}$                                                  &  &  \\
SVM      			& $\epsilon$-insensitive	& Elasticnet       & $10^{-2}$                                              &  &  \\
SVM              		& Hinge 		& L2               & $10^{-3}$                                                 &  &  \\
SVM       			& Squared hinge		& L2               & $10^{-3}$                                                 &  &  \\
SVM             		& Squared		& Elasticnet       & $10^{-2}$                                              &  &  \\
Ridge               		& Squared		 & L2               & $10^{-4}$                                             &  &  \\
Logistic Regression             & Log			& L2               & $10^{-1}$                                                   &  &  \\
Passive Aggressive		& Hinge 		&             & $10^{-2}$                                                  &  &  \\
Perceptron                      &			& Elasticnet             & $10^{-4}$                                            &  &  \\
Multilayer Perceptron          & Log			& Elasticnet       & $10^{-6}$                                                        &  &  \\
Neural Networks                 & Cross-entropy		& L2               & $10^{-4}$ &  &  \\ 
Naive Bayes                    	& 			 &              & $10^{-3}$ (Laplace smoothing)                            &  &  \\\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Unsupervised Reranking}

\subsubsection{Linear Combination}
Table \ref{interpolation-res} lists the absolute P@10 improvements of the reranked results, using the linear combination of BM25 and classifier scores (formula \ref{interp-formula}),
with the best $\lambda$ being $0.7\pm 0.02$.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\caption{Absolute P@10 improvement over the baseline, using weighted \textbf{linear combination fusion} of the baseline BM25 scores and the classifier scores.}
\label{interpolation-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\\midrule
Baseline                             & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
SVM (with Precision@K loss)                                 & +8                       & +9                          & +\textbf{4}              & +3.33                       & +\textbf{6.08}    \\ 
SVM (with Rec@K loss)                                    & +\textbf{8.33}           & +\textbf{10}                & +3                       & +2.67                       & +6                \\ 
Ridge                                          & +7                       & +9                          & +3.33                    & +\textbf{4}                 & +5.83             \\ 
SVM (with ROC area loss)                                    & +7.67                    & +9.67                       & +2                       & +3.67                       & +5.75             \\ 
SVM (with Error-rate loss)                                 & +8                       & +8.67                       & +3                       & +3                          & +5.67             \\ 
SVM (with Squared hinge loss)                           & +7                       & +9                          & +3                       & +3.67                       & +5.67             \\ 
Logistic Regression                            & +7.67                    & +8.67                       & +3                       & +3                          & +5.59             \\ 
SVM (with Squared loss)                               & +7.67                    & +9.33                       & +3.33                    & +1.67                       & +5.5              \\ 
SVM (with F1 loss)                                       & +7                       & +8.67                       & +2.67                    & +3.33                       & +5.42             \\ 
SVM (with Zero/one loss)                                        & +7                       & +8.67                       & +3                       & +3                          & +5.42             \\ 
Passive Aggressive                             & +6                       & +8                          & +2.33                    & +4                          & +5.08             \\ 
SVM (with $\epsilon$-insensitive loss)                            & +6.33                    & +7.67                       & +2.67                    & +3                          & +4.92             \\ 
SVM (with Hinge loss)                                     & +5.33                    & +8                          & +2                       & +3.67                       & +4.75             \\ 
Multilayer Perceptron                           & +7.67                    & +8                          & +1.67                    & +1                          & +4.59             \\ 
Neural Networks                               & +3                       & +5.67                       & +2.33                    & 0                          & +2.75             \\ 
Perceptron                                      & +4.33                    & +4.33                       & +0.33                    & 0                          & +2.25             \\ 
Naive Bayes                                      & +3                       & +4                          & +1                       & -1                         & +1.75             \\ \bottomrule
\end{tabular}%
}
\end{table}

The best rerankers are the ones using SVM classifiers, with the top one giving an overall improvement of over 6 percentage
points in P@10. For 2014, the best classifier was SVM with Rec@K loss (8.33\% and 10\% improvements), 
and for 2015 they were SVM with Precision@K loss and Ridge (4\% improvements).

In particular, for summaries 2014, the top reranked result is $31.67\%+8.33\% = 40\%$, which is better than the top automatic submission
from TREC 2014 (39\%).

Generally, the improvements for 2014 are at least 
twice as high as the ones for 2015, but this may be due to the lower baseline precisions from 2014 compared to 2015.

Looking more closely, we can see that all the top 10 classifiers are not more than 0.7\% worse off compared to the top one. However, the bottom three
are clearly worse, yielding improvements less than 3\%. In the case of neural networks, the problem might be the small size of the training data,
while the Perceptron and Bayes classifiers are too naive for this kind of task.

\subsubsection{Reciprocal Rank Fusion}
Table \ref{rrf-res} lists the absolute P@10 improvements of the reranked results, using Reciprocal Rank Fusion of the BM25 and classifier ranks (formula \ref{rrf-formula}),
with the best $\lambda$ being $0.6\pm 0.02$.
Similar to the linear combination method, SVM gave the best results, although the best one here uses a squared loss instead of 
Precision@K (which came out fourth).

However, the RRF improvements are overall lower than for linear combination: the best improvement is 4.75\% compared to 6\%, respectively. One reason
might be that RRF loses some information by not using the actual scores, but just the ranks instead.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\caption{Absolute P@10 improvement over the baseline, using weighted \textbf{reciprocal rank fusion} of the baseline BM25 scores and the classifier scores.}
\label{rrf-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
                                    & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\ \midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule

SVM (with Squared loss)                               & +5                       & +7.33                       & +3.67                    & +\textbf{3}                          & +\textbf{4.75}             \\ 
SVM (with Rec@K loss)                                                                      & +5                       & +\textbf{7.67}                       & +{3}              & +2                          & +{4.42}    \\ 
Passive Aggressive                                       & +\textbf{5.33}           & +5.67              & +2.33                    & +2.67                       & +4                \\ 
SVM (with Precision@K loss)                                                                 & +3.67                    & +7                          & +3.33                    & +{1.67}              & +3.92             \\ 
SVM (with F1 loss)                                                                          & +4.33                    & +6.33                       & +2.67                    & +2.33                       & +3.92             \\ 
Logistic Regression                                                      & 3.67                    & +7                          & +2.67                    & +2                          & +3.84             \\ 
SVM (with Hinge loss)                                                                        & +4                       & +5.67                       & +2.67                    & +\textbf{3}                          & +3.84             \\ 
SVM (with Error-rate loss)                                                                    & +4.33                    & +6                          & +3.33                    & +1.67                       & +3.83             \\ 
SVM (with Squared hinge loss)                                                              & +3.67                    & +6.33                       & +2.33                    & +2.67                       & +3.75             \\ 
Ridge                                                             & +3.33                    & +7                          & +2.33                    & +2                          & +3.67             \\ 
SVM (with $\epsilon$-insensitive loss)                         & +4.33                    & +6.67                       & +1.67                    & +1.67                       & +3.59             \\ 
SVM (with ROC area loss)                                                                    & +3.33                    & +7                          & +2.33                    & +1.67                       & +3.58             \\ 
SVM (with Zero/one loss)                                                                        & +4                       & +5.67                       & +2.33                    & +2.33                       & +3.58             \\ 
Multilayer Perceptron								& +4 & +6.33 & +2 & +0 & +3.08 \\
Naive Bayes                                                                      & +5                       & +5.67                       & +1.33                    & -1                         & +2.75             \\ 
Neural Networks                                                                   & +3.67                    & +4.67                       & 0                       & +1                          & +2.34             \\ 
Perceptron                                                              & +2                       & +3                          & +1                       & 0                          & +1.5              \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Borda Fuse}
The results using the Borda Fusion algorithm are listed in table \ref{borda-res}, with the optimal weight being $\lambda=0.6\pm0.05$.
Similar to RRF, SVM with squared loss is the reranker that gives the best improvement. However, the results for Borda are lower
than RRF by around 0.5\%.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h]
\centering
\caption{Absolute P@10 improvement over the baseline, using weighted \textbf{Borda fusion} of the baseline BM25 scores and the classifier scores.}
\label{borda-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
                                       & Summaries 2014 & Descriptions 2014 & Summaries 2015 & Descriptions 2015 & Average       \\\midrule
Baseline                                   & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule
                                   
SVM (with Squared loss)                               & +\textbf{4.67}  & +\textbf{8.67}     & +2.33           & +\textbf{1.33}     & +\textbf{4.25} \\
SVM (with Precision@K loss)                            & +4              & +9                 & +1.67           & +\textbf{1.33}     & +4             \\
SVM (with Rec@K loss)                                 & +4.33           & +7.67              & +3.33           & +0.67              & +4             \\
Logistic Regression                                    & +4.33           & +8                 & +2              & +\textbf{1.33}     & +3.92          \\
SVM (with Error-rate loss)                             & +4.33           & +6                 & +3.33           & +1                 & +3.67          \\
SVM (with F1 loss)                                     & +4              & +5.67              & +4              & +1                 & +3.67          \\
SVM (with Squared hinge loss)                           & +3              & +8                 & +2              & +\textbf{1.33}     & +3.58          \\
SVM (with Zero/one loss)                               & +3.67           & +5.67              & +\textbf{3.67}  & +1                 & +3.5           \\
Ridge                                                   & +3              & +8                 & +1.33           & +1                 & +3.33          \\
SVM (with ROC area loss)                                & +3.67           & +8                 & +1.33           & +0.33              & +3.33          \\
SVM (with $\epsilon$-insensitive loss)                   & +3.33           & +7                 & +1.33           & +1                 & +3.17          \\
Passive Aggressive                                       & +3.67           & +5                 & +1.67           & +1                 & +2.84          \\
SVM (with Hinge loss)                                    & +2              & +6.33              & +0.67           & +1                 & +2.5           \\
Multilayer Perceptron                                 & +2.67           & +6.33              & +1              & -1                & +2.25          \\
Naive Bayes                                              & +3.33           & +6.33              & +0.33           & -3                & +1.75          \\
Neural Networks                                         & +1              & +4.33              & -1             & -1                & +0.83          \\
Perceptron                                              & +1.33           & +2                 & +0.67           & +-2.33             & +0.42         \\\bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Weighting analysis}

Figures \ref{interp-weight}, \ref{rrf-weight} and \ref{borda-weight} show how the improvement in P@10 behaves as the
weight is varied between 0 and 1. A weight of 0 means only the classifier scores count and a weight of 1 means
only the baseline BM25 counts.

The highest improvements happen in the [0.6, 0.8] interval, after which the P@10 starts to converge towards the baseline.
We can also re-confirm that the 2014 improvements are much higher than the ones from 2015, and in some cases (e.g., Descriptions
2014), the classifier alone is better than the baseline.

Even though linear combination is the best on average, RRF and Borda are better for 2014. However, the weight for which they are
best for 2014 (around 0.4) would make 2015 worse than the baseline.

\begin{figure}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/interpolation/all/all.SVMPerf.04.0.001.hedges.png}
  }
  \caption{P@10 improvements using linear combination and SVM with Precision@K loss function.}
  \label{interp-weight}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/rrf/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
  \caption{P@10 improvements using RRF and SVM with squared loss function.}
  \label{rrf-weight}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/borda/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
  \caption{P@10 improvements using Borda fusion and SVM with squared loss function.}
  \label{borda-weight}
\end{figure}

\subsubsection{Effect of the keyword counter}
\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/basic-vs-non-basic.png}
  }
  \caption{Effect of using the keyword counter.}
  \label{basic-effect-plot}
\end{figure}

\subsubsection{Improvement per query}

Figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query} show the increase or decrease in P@10 for each query,
using linear interpolation, RRF and Borda fuse, respectively. All four cases are shown (summaries and descriptions for 2014 and 2015).

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SVMPerf.04.0.001.hedges.interpolation.png}
  }
  \caption{P@10 improvements per query using linear combination and SVM with Recall@K loss function.}
  \label{interp-query}
\end{figure}

For the linear interpolation method, 16, 18, 14 and 10 queries have increased and 3, 1, 6 and 4 have decreased
for the four cases, respectively. For 2015, the highest overall increase was by fat for treatment queries (21 to 30).

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.rrf.png}
  }
  \caption{P@10 improvements per query using RRF and SVM with squared loss function.}
  \label{rrf-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.borda.png}
  }
  \caption{P@10 improvements per query using Borda fusion and SVM with Squared loss function.}
  \label{borda-query}
\end{figure}


\subsection{Supervised reranking}

We now present the results of the supervised reranking strategies. We use several learning to rank algorithms, we present the ones that have the best results.
As described earlier, when reranking the 2014 queries, we use the 2015 ones as training set and vice versa.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Parameters}
\label{my-label}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|l@{}}
\toprule
Algorithm         & \textbf{Parameters}                                                             \\\midrule
RankNet           & 40 iterations, 1 hidden layer, 10 nodes/layer                                   \\
RankBoost         & 40 iterations                                                                   \\
AdaRank           & 10 iterations, max number of times a feauture can be consecutively selected: 2  \\
Coordinate Ascent      & 20 iterations, Regularization parameter: $10^{-3}$                  \\
MART              & 100 trees, 10 leaves                                                            \\
Lambda MART       & 60 trees, 10 leaves                                                             \\
Linear Regression & Regularization parameter: $10^{-10}$      				\\\bottomrule                        
\end{tabular}%
}
\end{table}

Here are the overall results:

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\caption{Learning to Rank results.}
\label{l2r-res}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
\toprule
Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule

Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule
Baseline          & \multicolumn{2}{c|}{31.67}          & \multicolumn{2}{c|}{25}              & \multicolumn{2}{c|}{38.67}     & \multicolumn{2}{c|}{35}     &32.59         \\ \midrule

Linear Regression & \textbf{+7}       & +6.67             & \textbf{+8.33}   & +7.67              & +2.67       & +1.67    	& +1          & \textbf{+2.33}    & +\textbf{5.08}  \\
AdaRank           & +6.33             & +3.33             & +8               & +6.67              & \textbf{+3} & +1.67         & +1.33       & \textbf{+2.33}    & +4.91  \\
RankNet           & +6.33             & +7.33             & +6               & +6                 & -3.67       & +1.67         & -2.33       & +1.67             & +4.16  \\
Coordinate Ascent & +6.33             & +6                & +8               & +5.33              & -3.67       & +1            & -3          & -0.3              & +3.75  \\
Lambda MART	  & +4		      & +2.33		  & +4		     & +6		  & -7.33	& -1.33		& -5.33	      & -0.3		  & +2.34  \\		
MART	 	  & +5.67	      & +2		  & +6.33	     & +2.33		  & -4.33	& -4		& -6.77	      & -5		  & +0.75 \\			
RankBoost	  & +4.67	      & +3		  & +4		     & +5		  & -5.67	& -3		& -7	      & -4.67		  & +0.67 \\
% Random Forests    & +5.33             & -1.67             & +5.33            & +3.67              & -5          & -1.67         & -5.33       & -2.33             & 1.67  \\


\bottomrule
\end{tabular}%
}
\end{table}


\begin{table}[h!]
\caption{Learning to Rank Percentage of unjudged documents in Top 10.}
\label{l2r-res}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
\toprule
Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule

Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule

Linear Regression & 4              & 5                 & 10               & 12                 & 4              & 1                 & 8                & 7        & 6.25          \\
AdaRank           & 4              & 4                 & 12               & 10                 & 1              & 1                 & 9                & 7        & 6         \\
RankNet           & 9              & 8                 & 17               & 17                 & 9              & 1                 & 11               & 7        & 8.25         \\
Coordinate Ascent & 4              & 7                 & 12               & 16                 & 9              & 5                 & 12               & 8        & 7.25         \\
Lambda MART       & 11             & 9                 & 17               & 19                 & 11             & 4                 & 16               & 11       & 11.25         \\
MART              & 9              & 1                 & 15               & 19                 & 9              & 8                 & 14               & 15       & 11.75       \\
RankBoost         & 11             & 12                & 19               & 20                 & 11             & 8                 & 16               & 13       & 13       \\

\bottomrule
\end{tabular}%
}
\end{table}


\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.3]{../../ranking/plots/unjudged.png}
  }
  \caption{Unjudged docs.}
  \label{unjudged-fig}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.27]{../../ranking/plots/bar/regression.regression.png}
  }
  \caption{P@10 improvements per query using linear regression for reranking.}
  \label{regression-query}
\end{figure}

\subsection{Comparison with prev. paper}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{Comparison with prev. paper}
\label{my-label}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
                  & \textbf{Results prev. paper results} & \textbf{Prev. paper method, our results} & \textbf{Our best method} \\\midrule
Summaries 2014    & 33.67\tablefootnote{Without re-ranking diagnosis queries, results is 34.67} (32 + 1.67)                   & 36.33 (31.67 +4.67)                      & 40 (31.67 +8.33)         \\
Descriptions 2014 & 33.00\tablefootnote{Without re-ranking diagnosis queries, results is 36.33} (32 + 1)                      & 32.00 (25 + 7)                           & 35 (25+10)              \\
\bottomrule
\end{tabular}%
}
\end{table}





