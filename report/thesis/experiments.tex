\chapter{Experimental Results}\label{exp}

Let us now present the experimental results of applying our system formally defined in Chapter \ref{meth} and implemented in Chapter
\ref{impl}
to the TREC 2014 and 2015 Clinical Decision Support tracks.
We first discuss the evaluation metric and the classifier parameters,
and then proceed to analyzing
the results of the unsupervised (Linear Combination, RRF and Borda)
and supervised (Learning to Rank) fusion methods. We then conduct an overall comparison between the best results from each strategy
and, finally, we compare our best overall results with the results from Choi's et al. previous research~\cite{choi}.

\section{Query Runs}\label{runs} Before we begin, let us recall from Section \ref{sim-impl} that we have four \emph{runs} with 30 queries each:
\begin{itemize}
 \item using the query \emph{summaries} from \emph{2014};
 \item using the query \emph{descriptions} from \emph{2014};
 \item using the query \emph{summaries} from \emph{2015};
 \item using the query \emph{descriptions} from \emph{2015};
\end{itemize}

We present a set of results for each of these runs and also three \emph{mean results}:
\begin{itemize}
 \item the mean for 2014 (summaries and descriptions);
 \item the mean for 2015 (summaries and descriptions);
 \item the overall mean over all four runs.
\end{itemize}

\section{Evaluation Metric}
To evaluate the quality of the rankings produced by our scoring function, we use the Precision@10 metric (P@10), which counts for each query
the percentage of
relevant results from the top 10 retrieved results (for example, if four documents are relevant from the top 10 retrieved,
then P@10 equals 40\%). The P@10 is then averaged over all queries in a run.

As discussed in Section \ref{qrels}, the ground truths --- relevance judgments --- are provided by medical experts
and stored in the (\emph{qrels}) files.

\section{Baseline and Reranked Results}
For each of the four query runs listed in Section \ref{runs}, we compute:
\begin{itemize}
 \item a \emph{baseline} P@10, using only \textsf{Relevance} scores (BM25) for retrieval;
 \item a \emph{reranked} P@10,
 using a \textsf{Fusion} of \textsf{Relevance} and \textsf{TypeClassifier} scores --- as described in Chapters \ref{meth} and \ref{impl} --- for retrieval.
\end{itemize}
The \emph{absolute improvement} in P@10 is the difference between the reranked P@10 and the baseline P@10 (Formula \ref{improvement-formula}).

\section{Classifier Parameters}
Table \ref{params} lists the 17 classifiers we use and their configurations.
We empirically determine
the
parameters by varying them in a certain range (for example, the regularization parameters
in the $[10^{-4},\cdots,10^4]$ range, in steps of powers of 10) and choosing the ones
that give the
best P@10 improvement averaged over all four categories.

For the Multilayer Perceptron classifier, we achieved the best results using two layers of size 50 and 5 respectively
(we varied the layer sizes from 5 to 100 and the number of layers from 1 to 3).

For the Convolutional Neural Networks classifier, we achieved the best results
using 100 convolution filters of size 3, 4 and 5, respectively and 50 nodes in last hidden layer
(we varied the number of filters from 50 to 150
and the number of nodes in the last hidden layer from 0, i.e., no hidden layer, to 75); the word-embedding size is 100.

For Formula \ref{clf-combine-formula}, we achieved the best results using
$\alpha=0.4$ and $\textsf{cutoff}=4$ (we varied $\alpha$  between 0 and 1 and \textsf{cutoff} between 1 and 10).

All classifier parameters not listed in table \ref{params} (e.g., tolerance) are set to their default values (from \texttt{scikit-learn} or \texttt{SVM-perf}).

\begin{table}[h!]
\centering
\caption{Best classifier parameters found (penalty and regularization).}
\label{params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Classifier}             & \textbf{Loss function}  & \textbf{Penalty} & \textbf{Regularization parameter}                             &  &  \\ \midrule
SVM               		& Precision@K		 & L2               & $10^{-3}$                                                 &  &  \\
SVM               		& Recall@K		 & L2               & $10^{-3}$                                               &  &  \\
SVM           			& ROC area		 & L2               & $10^{-3}$                                                &  &  \\
SVM           			& Error-rate		& L2               & $1$                                                     &  &  \\
SVM                 		& F1			& L2               & $1$                                                     &  &  \\
SVM           			& Zero/one		& L2               & $10^{-2}$                                                  &  &  \\
SVM      			& $\epsilon$-insensitive	& Elasticnet       & $10^{-2}$                                              &  &  \\
SVM              		& Hinge 		& L2               & $10^{-3}$                                                 &  &  \\
SVM       			& Squared hinge		& L2               & $10^{-3}$                                                 &  &  \\
SVM             		& Squared		& Elasticnet       & $10^{-2}$                                              &  &  \\
Ridge               		& Squared		 & L2               & $10^{-4}$                                             &  &  \\
Logistic Regression             & Log			& L2               & $10^{-1}$                                                   &  &  \\
Passive Aggressive		& Hinge 		&             & $10^{-2}$                                                  &  &  \\
Perceptron                      &			& Elasticnet             & $10^{-4}$                                            &  &  \\
Multilayer Perceptron          & Log			& Elasticnet       & $10^{-6}$                                                        &  &  \\
Neural Networks                 & Cross-entropy		& L2               & $10^{-4}$ &  &  \\
Naive Bayes                    	& 			 &              & $10^{-3}$ (Laplace smoothing)                            &  &  \\\bottomrule
\end{tabular}%
}
\end{table}

\section{Unsupervised Fusion Results}
Let us now present the results of the three unsupervised fusion methods (Linear Combination, RRF and Borda).

For each of the four runs listed in Section \ref{runs} and for each of the seventeen classifiers listed in Table \ref{params},
we perform a fusion of scores
between the baseline BM25 \textsf{Relevance} scores (Formula \ref{bm25-formula})
and the \textsf{TypeClassifier} scores (Formula \ref{clf-combine-formula}), as described in Sections \ref{unsup-fusion-theory}
and \ref{impl-unsup}.
We therefore produce seventeen sets of results for each of the four runs,
one set of results per classifier.
We then average for each classifier the results of the corresponding four runs, producing seventeen \emph{average results}.
This shows which classifier leads to the highest average P@10 improvement for that fusion method.

Furthermore, we inspect the contribution of the keyword counter towards the overall P@10 improvement. More precisely,
we look at what the improvement is \emph{with} (default) versus \emph{without} using the keyword counter, that is, using $\alpha=0.4$
versus using $\alpha=0$, respectively, in Formula \ref{clf-combine-formula}.

Lastly, using the best classifier, we show for each run the improvement for each of the 30 queries individually.

\subsection{Linear Combination Results}
We begin by showing the results of the Linear Combination fusion method (Formula \ref{interp-formula}) in Table \ref{interpolation-res}.
The weight $\lambda$ is set to $0.7\pm 0.02$.

The first line shows the \emph{baseline} P@10 for each of the four runs.
The next lines show the \emph{absolute} P@10 \emph{improvements} for each classifier.

\begin{table}[h!]
\centering
\caption{Absolute P@10 percent improvements per classifier for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using weighted \textbf{Linear Combination}
for fusing the baseline relevance scores with the classifier scores.}
\label{interpolation-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summ. 2014} & \textbf{Desc. 2014} & \textbf{Summ. 2015} & \textbf{Desc. 2015} & \textbf{Average} \\\midrule
Baseline P@10 (\%)                          	& 31.67                    & 25                          & 38.67                    & 35                          &   32.59               \\ \midrule
SVM (with Precision@K loss)        	& +8                       & +9                          & +\textbf{4}              & +3.33                       & +\textbf{6.08}    \\
SVM (with Rec@K loss)              	& +\textbf{8.33}           & +\textbf{10}                & +3                       & +2.67                       & +6                \\
Ridge                                	& +7                       & +9                          & +3.33                    & +\textbf{4}                 & +5.83             \\
SVM (with ROC area loss)             	& +7.67                    & +9.67                       & +2                       & +3.67                       & +5.75             \\
SVM (with Error-rate loss)         	& +8                       & +8.67                       & +3                       & +3                          & +5.67             \\
SVM (with Squared hinge loss)     	& +7                       & +9                          & +3                       & +3.67                       & +5.67             \\
Logistic Regression               	& +7.67                    & +8.67                       & +3                       & +3                          & +5.59             \\
SVM (with Squared loss)         	& +7.67                    & +9.33                       & +3.33                    & +1.67                       & +5.5              \\
SVM (with F1 loss)                	& +7                       & +8.67                       & +2.67                    & +3.33                       & +5.42             \\
SVM (with Zero/one loss)            	& +7                       & +8.67                       & +3                       & +3                          & +5.42             \\
Passive Aggressive              	& +6                       & +8                          & +2.33                    & +4                          & +5.08             \\
SVM (with $\epsilon$-insensitive loss)	& +6.33                    & +7.67                       & +2.67                    & +3                          & +4.92             \\
SVM (with Hinge loss)              	& +5.33                    & +8                          & +2                       & +3.67                       & +4.75             \\
Multilayer Perceptron            	& +7.67                    & +8                          & +1.67                    & +1                          & +4.59             \\
Neural Networks                  	& +3                       & +5.67                       & +2.33                    & 0                           & +2.75             \\
Perceptron                          	& +4.33                    & +4.33                       & +0.33                    & 0                           & +2.25             \\
Naive Bayes                        	& +3                       & +4                          & +1                       & -1                          & +1.75             \\ \bottomrule
\end{tabular}%
}
\end{table}

The best rerankers are the ones using SVM classifiers, with the top two giving an average improvement in P@10 of at least 6 percentage
points. For Summaries and Descriptions 2014, the best classifier is SVM with Recall@K loss (8.33\% and 10\% improvements),
for Summaries 2015, the best classifier is SVM with Precision@K loss (4\% improvement),
and for Descriptions 2015, the best classifier is Ridge (4\% improvement).

In particular, for Summaries 2014, the top reranked result is $31.67\%+8.33\% = 40\%$, which is better than the top automatic submission
from TREC 2014 (39\%).

Generally, the improvements for 2014 are at least
twice as high as the ones for 2015, but this may be due to the lower baseline precisions from 2014 compared to 2015.

Looking more closely, we see that the top 10 classifiers are not more than 0.7\% worse off compared to the top one,
which means that any one of them could perform well with other queries.

However, the bottom three classifiers are more modest, yielding improvements of less than 3\%.
In the case of neural networks, the problem might be the small size of the training data,
while the Perceptron and Bayes classifiers are too naive for this task.
Nevertheless, they still give some moderate improvements for 2014 and Summaries 2015.

\subsection{Reciprocal Rank Fusion Results}
Let us now show the results of the Reciprocal Rank Fusion (RRF) method (Formula \ref{rrf-formula}) in Table \ref{rrf-res}.
The weight $\lambda$ is set to $0.6\pm 0.02$ and $k$ is set to 60.

\begin{table}[h!]
\centering
\caption{Absolute P@10 percent improvements per classifier for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using weighted \textbf{Reciprocal Rank Fusion}
for fusing the baseline relevance scores with the classifier scores.}
\label{rrf-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
                                    & \textbf{Summ. 2014} & \textbf{Desc. 2014} & \textbf{Summ. 2015} & \textbf{Desc. 2015} & \textbf{Average} \\ \midrule
Baseline P@10 (\%)                     		& 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule

SVM (with Squared loss)         		& +5                       & +7.33                       & +\textbf{3.67}           & +\textbf{3}                 & +\textbf{4.75}             \\
SVM (with Rec@K loss)                  		& +5                       & +\textbf{7.67}         	 & +{3}                     & +2                          & +{4.42}    \\
Passive Aggressive                         	& +\textbf{5.33}           & +5.67             		 & +2.33                    & +2.67                       & +4                \\
SVM (with Precision@K loss)                   	& +3.67                    & +7                          & +3.33                    & +{1.67}              	  & +3.92             \\
SVM (with F1 loss)                          	& +4.33                    & +6.33                       & +2.67                    & +2.33                       & +3.92             \\
Logistic Regression                        	& 3.67                     & +7                          & +2.67                    & +2                          & +3.84             \\
SVM (with Hinge loss)                      	& +4                       & +5.67                       & +2.67                    & +\textbf{3}              	  & +3.84             \\
SVM (with Error-rate loss)                   	& +4.33                    & +6                          & +3.33                    & +1.67                       & +3.83             \\
SVM (with Squared hinge loss)                	& +3.67                    & +6.33                       & +2.33                    & +2.67                       & +3.75             \\
Ridge                                      	& +3.33                    & +7                          & +2.33                    & +2                          & +3.67             \\
SVM (with $\epsilon$-insensitive loss)       	& +4.33                    & +6.67                       & +1.67                    & +1.67                       & +3.59             \\
SVM (with ROC area loss)                    	& +3.33                    & +7                          & +2.33                    & +1.67                       & +3.58             \\
SVM (with Zero/one loss)                       	& +4                       & +5.67                       & +2.33                    & +2.33                       & +3.58             \\
Multilayer Perceptron				& +4 			   & +6.33 			 & +2 			    & +0 			  & +3.08 \\
Naive Bayes                                    	& +5                       & +5.67                       & +1.33                    & -1                          & +2.75             \\
Neural Networks                              	& +3.67                    & +4.67                       & 0                        & +1                          & +2.34             \\
Perceptron                                     	& +2                       & +3                          & +1                       & 0                           & +1.5              \\ \bottomrule
\end{tabular}%
}
\end{table}

Similar to the linear combination method, SVM classifiers generally work best,
although the top one now uses the squared loss instead of Precision@K loss (which takes fourth place instead).

RRF, however, gives lower improvements, with the highest one being 4.75\%
(compared to 6\% for linear combination).
This may be due to the fact that RRF loses some information by only using document ranks instead of raw scores. On the other hand, RRF
is easier and more convenient to compute, since no score normalization is needed for \textsf{Relevance} and \textsf{TypeClassifier}.


\subsection{Borda Fusion Results}
We show the results of the Borda fusion method (Formula \ref{borda-formula}) in Table \ref{borda-res}.
The weight $\lambda$ is set to $0.6\pm 0.05$.

\begin{table}[h!]
\centering
\caption{Absolute P@10 percent improvements per classifier for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using weighted \textbf{Borda Fusion}
for fusing the baseline relevance scores with the classifier scores.}
\label{borda-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
                                    & \textbf{Summ. 2014} & \textbf{Desc. 2014} & \textbf{Summ. 2015} & \textbf{Desc. 2015} & \textbf{Average} \\ \midrule
Baseline P@10 (\%)                                 & 31.67                   & 25                         & 38.67                   & 35                         &   32.59               \\ \midrule

SVM (with Squared loss)                               & +\textbf{4.67}  & +8.67     & +2.33           & +\textbf{1.33}     & +\textbf{4.25} \\
SVM (with Precision@K loss)                            & +4              & +\textbf{9}                 & +1.67           & +\textbf{1.33}     & +4             \\
SVM (with Rec@K loss)                                 & +4.33           & +7.67              & +3.33           & +0.67              & +4             \\
Logistic Regression                                    & +4.33           & +8                 & +2              & +\textbf{1.33}     & +3.92          \\
SVM (with Error-rate loss)                             & +4.33           & +6                 & +3.33           & +1                 & +3.67          \\
SVM (with F1 loss)                                     & +4              & +5.67              & \textbf{+4}     & +1                 & +3.67          \\
SVM (with Squared hinge loss)                           & +3              & +8                 & +2              & +\textbf{1.33}     & +3.58          \\
SVM (with Zero/one loss)                               & +3.67           & +5.67              & +3.67		  & +1                 & +3.5           \\
Ridge                                                   & +3              & +8                 & +1.33           & +1                 & +3.33          \\
SVM (with ROC area loss)                                & +3.67           & +8                 & +1.33           & +0.33              & +3.33          \\
SVM (with $\epsilon$-insensitive loss)                   & +3.33           & +7                 & +1.33           & +1                 & +3.17          \\
Passive Aggressive                                       & +3.67           & +5                 & +1.67           & +1                 & +2.84          \\
SVM (with Hinge loss)                                    & +2              & +6.33              & +0.67           & +1                 & +2.5           \\
Multilayer Perceptron                                 & +2.67           & +6.33              & +1              & -1                & +2.25          \\
Naive Bayes                                              & +3.33           & +6.33              & +0.33           & -3                & +1.75          \\
Neural Networks                                         & +1              & +4.33              & -1             & -1                & +0.83          \\
Perceptron                                              & +1.33           & +2                 & +0.67           & +-2.33             & +0.42         \\\bottomrule
\end{tabular}%
}
\end{table}

Again, SVM classifiers work best, the top one using the squared loss, like RRF.
However, the results for Borda are generally lower than for RRF by around 0.5\%. This is mostly due
to poorer results for 2015 (1.83\% vs. 3.33\% for the top result).
On the other hand, Borda works slightly better for 2014 (6.67\% vs 6.16\% for the top result).

Overall, the results of the three fusion methods indicate that \[\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}.\]

\subsection{Relation between Weight $\lambda$ and Improvement}
Let us now investigate the relation between weight $\lambda$ and the improvement in P@10 for each of the three unsupervised
fusion methods, when using their best classifier (i.e., SVM with Precision@K loss for linear combination
and SVM with squared loss for RRF and Borda).
Recall from Formulas \ref{interp-formula}, \ref{rrf-formula} and \ref{borda-formula} that the weight $\lambda$ is the
weight of the baseline \textsf{Relevance} score (or rank) and $1-\lambda$ is the weight of the \textsf{TypeClassifier} score (or rank).

Figure \ref{lambda-vary} shows for each of the three methods
how the improvement in P@10 behaves as the weight $\lambda$ is varied between 0 and 1 in steps of 0.01.

\begin{figure}[h!]

\begin{subfigure}[b]{\textwidth}
 \centerline {
  \includegraphics[scale=0.22]{../../ranking/plots/interpolation/all/all.SVMPerf.04.0.001.hedges.png}
  }
%   \vspace{-0.1cm}
%   \caption{P@10 improvements using linear combination and SVM with Precision@K loss function.}
  \label{interp-weight}
\end{subfigure}
%  \vspace{0.5cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/rrf/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using RRF and SVM with squared loss function.}
  \label{rrf-weight}
\end{subfigure}
% \vspace{0.2cm}
\begin{subfigure}[b]{\textwidth}
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/borda/all/all.SGDClassifier.squared_loss.elasticnet.hedges.png}
  }
%   \caption{P@10 improvements using Borda fusion and SVM with squared loss function.}
  \label{borda-weight}
\end{subfigure}

 \caption{Variation in the absolute P@10 percent improvement, as weight $\lambda$ is varied between 0 and 1, for each of the three unsupervised
fusion methods --- Linear Combination, RRF and Borda Fuse --- using their best classifier (Tables \ref{interpolation-res}, \ref{rrf-res}
and \ref{borda-res}, respectively).}
 \label{lambda-vary}

\end{figure}

The first observation to make in Figure \ref{lambda-vary} is that the highest improvements generally happen for $\lambda \in [0.6, 0.8]$.
As also reflected in Tables \ref{interpolation-res}, \ref{rrf-res} and \ref{borda-res}, the 2014 improvements are much higher than
the 2015 ones.

Interestingly, for Descriptions 2014, using $\lambda = 0$ gives better results
than using $\lambda=1$. This does not mean, however, that for 2014, using the classifier scores
works better than using the BM25 scores, but rather that
\emph{re-sorting} the top 100 BM25 results by their classifier scores is better.
We still need the BM25 retrieval step to work as a ``filter'',
since otherwise the top retrieved documents would be the ones with the highest classifier scores out of the whole document collection,
regardless of whether
they have any \emph{textual} similarity to the query --- this is also discussed in Chapter \ref{meth}.
In any case,  however, this peculiarity only happens for 2014; for 2015 the improvements happen only when $\lambda>0.5$,
and even for 2014, the highest improvements happen when $\lambda>0.3$.

Moving forward with the analysis of Figure \ref{lambda-vary}, we see that
the improvements for 2014 and 2015 are more correlated with each other when using linear combination than when using
RRF or Borda. By this we mean that the two weights that give the peak average improvements for 2014 and 2015 are closer to each other
when using linear combination (both around $0.7$) than they are when using RRF or Borda ($0.4$ vs $0.6$ for 2014 and 2015, respectively).
This ``mismatch'' might also contribute to the lower average peak precision for RRF and Borda ($<4.8\%$)
vs. Linear Combination ($6\%$), despite the fact that they sometimes
give higher peak precisions for 2014 (e.g. peak Descriptions 2014 improvement
is $\sim10\%$ for Borda vs. only $\sim9\%$ for Linear combination). However, the 2015 improvements are always higher for
Linear Combination.

Looking beyond these granular differences and more at the overall picture, we can safely conclude
that choosing any of the three unsupervised fusion methods and any weight $\lambda$ between 0.6 and 0.8 gives
a solid improvement over the baseline.

\subsection{Contribution of the Keyword Counter}
Recall from Formula \ref{clf-combine-formula} that the classifier score is actually
a linear combination of the machine learning classifier score (e.g., SVM) and the keyword counter score. The idea,
explained in Section \ref{intent-scoring},
was that the keyword counter would double-check the classifier and maybe correct its predictions by
increasing or lowering the scores depending on the frequency of certain keywords.

Figure \ref{basic-effect-plot}
confirms the fact that the keyword counter helped. The peak average P@10 improvement for linear combination is 1\% higher
when using it ($\alpha=0.4$ in Formula \ref{clf-combine-formula} --- default) than when not using it ($\alpha=0$), and the $\lambda$--P@10 curve is generally wider.

For RRF and Borda the effect is less pronounced --- probably because they use ranks instead of scores ---
but is still there nonetheless.

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.22]{../../ranking/plots/basic-vs-non-basic.png}
  }
  \caption{Contribution of the Keyword Counter towards the average absolute P@10 improvement for each of the three unsupervised
  fusion methods --- Linear Combination, RRF and Borda --- using their best classifier (Tables \ref{interpolation-res}, \ref{rrf-res}
and \ref{borda-res}, respectively).}
  \label{basic-effect-plot}
\end{figure}

\subsection{Improvement per Query and Intent Type}
We conclude our analysis of unsupervised fusion methods by examining the improvement per query and intent type.

Figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query} show the change (increase or decrease) in P@10 for each query,
using Linear Combination, RRF and Borda fusion, respectively, using the best classifier for each method (SVM with
Precision@K and squared losses, respectively) and for each of the four runs, \{Summaries, Descriptions\} $\times$ \{2014, 2015\}.

We summarize the findings in Table \ref{change-per-query-table}, which counts
how many queries are affected after the reranking.

\begin{table}[h!]
\centering
\caption{Number of positively/negatively affected queries for each of the three unsupervised fusion methods, using their best classifier (Tables \ref{interpolation-res}, \ref{rrf-res}
and \ref{borda-res}, respectively).}
\label{change-per-query-table}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
			& \textbf{Better 2014} & \textbf{Worse 2014} & \textbf{Better 2015} & \textbf{Worse 2015} \\ \midrule
Linear Combination 	& 34                   & 4                   & 24                   & 10                  \\
Reciprocal Rank Fusion  & 27                   & 3                   & 22                   & 10                  \\
Borda Fusion       	& 30                   & 9                   & 24                   & 20                  \\ \bottomrule
\end{tabular}%
}
\end{table}

This reinforces the idea that $\textsf{Linear Combination} \gg \textsf{RRF} > \textsf{Borda}$
and that 2014 is easier to rerank than 2015.

Looking more closely at Figures \ref{interp-query}, \ref{rrf-query}, \ref{borda-query}
it is clear that
treatment queries (21-30) fared \emph{much} better than the diagnosis queries (1-10)
which fared \emph{slightly} better than the test queries (11-20).
This may be due to having only about 1000 positive samples for training the diagnosis classifier vs. 8000 positive samples
for training the treatment classifier. Moreover, we did not have any training samples for the test classifier,
so we had to use the diagnosis samples instead.

We summarize these findings in Table \ref{change-per-query-table-lincomb},
where we list the improvements in P@10 for each intent type using the Linear Combination method with SVM--Precision@K
classifier.

\begin{table}[h!]
\centering
\caption{Absolute P@10 percent improvements per intent type for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using weighted \textbf{Linear Combination}
for fusing the baseline relevance scores with the classifier scores.
The classifier is SVM with Precision@K loss.}
\label{change-per-query-table-lincomb}
\resizebox{\textwidth}{!}{%

\begin{tabular}{@{}llllll@{}}
\toprule
          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\\midrule
Diagnosis & +4                      & +11                        & +2                      & +3                         & \textbf{+5}               \\
Test      & +10                     & +4                         & +2                      & +3                         & \textbf{+4.75}            \\
Treatment & +10                     & +12                        & +8                      & +4                         & \textbf{+8.5}             \\\midrule
Average   & +8                      & +9                         & +4                      & +3.33                      & \textbf{+6.08}           \\\bottomrule
\end{tabular}%

}
\end{table}

\begin{figure}
\centerline{
  \includegraphics[scale=0.215]{../../ranking/plots/bar/all.SVMPerf.04.0.001.hedges.interpolation.png}
  }
  \caption{Linear Combination - P@10 change per query when using SVM classifier with Precision@K loss.}
  \label{interp-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.215]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.rrf.png}
  }
  \caption{Reciprocal Rank Fusion - P@10 change per query when using SVM classifier with squared loss.}
  \label{rrf-query}
\end{figure}

\begin{figure}
\centerline{
  \includegraphics[scale=0.215]{../../ranking/plots/bar/all.SGDClassifier.squared_loss.elasticnet.hedges.borda.png}
  }
  \caption{Borda Fusion - P@10 change per query when using SVM classifier with squared loss.}
  \label{borda-query}
\end{figure}


\section{Supervised Fusion Results}
Let us now proceed to the supervised fusion results. As mentioned in Sections \ref{sup-fusion} and \ref{impl-sup}, we use several
learning-to-rank algorithms which we train on the queries from one year and test on the queries from the other year.

The features that we use are the baseline BM25 relevance scores (Formula \ref{bm25-formula}) and the type classifier scores
(Formula \ref{clf-combine-formula}), using an SVM
classifier with Precision@K loss (the best configuration, according to the unsupervised linear combination method).

Since the learning-to-rank algorithms have several parameters, we performed a search for the ones that give the best
results (for example, we varied the number of iterations or trees from 5 to 500 in steps of 10).
The best parameters found are listed in Table \ref{l2r-params}. All other parameters are set to their default values from \texttt{RankLib}.

\begin{table}[h!]
\centering
\caption{Best parameters found for the learning-to-rank algorithms.}
\label{l2r-params}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Algorithm}  & \textbf{Parameters}                                                             \\\midrule
RankNet           & 40 iterations, 1 hidden layer, 10 nodes/layer                                   \\
RankBoost         & 40 iterations                                                                   \\
AdaRank           & 10 iterations, max number of times a feature can be consecutively selected: 2  \\
Coordinate Ascent      & 20 iterations, Regularization parameter: $10^{-3}$                  \\
MART              & 100 trees, 10 leaves                                                            \\
Lambda MART       & 60 trees, 10 leaves                                                             \\
Linear Regression & Regularization parameter: $10^{-10}$      				\\\bottomrule
\end{tabular}%
}
\end{table}

Table \ref{l2r-res} lists the final results, with the first and second lines
specifying the test and training sets used. For example, for test queries \emph{Summaries 2014},
either the \emph{Summaries 2015} (S) or the \emph{Descriptions 2015} (D) training sets can be used.
We list the results for both training sets as two columns.

The \emph{Best Average} column shows the \emph{best} average of the four test query runs.
That is, for each run, we use the training set (either \emph{Summaries} or \emph{Descriptions}) that gives the highest
improvement. For example, the best average improvement for \emph{Linear Regression} uses the \emph{Summaries 2015} training set for the
\emph{Summaries 2014} and \emph{Descriptions 2014} test sets,
the \emph{Summaries 2014} training set for the \emph{Summaries 2015} test set, and
the \emph{Descriptions 2014} training set for the \emph{Descriptions 2015} test set.

For convenience, we \textit{italicize} the results contributing to the Best Average column.

\begin{table}[h!]
\caption{Absolute P@10 percent improvements for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using \textbf{supervised fusion (learning-to-rank)}
for fusing the baseline relevance scores with the classifier scores.
The classifier is SVM with Precision@K loss. The training queries are
either the Summaries \emph{(S)} or the Descriptions \emph{(D)} queries from the other year.}
\label{l2r-res}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllllll@{}}
\toprule



& \multicolumn{2}{c}{\textbf{Summaries 2014}} & \multicolumn{2}{c}{\textbf{Descriptions 2014}} & \multicolumn{2}{c}{\textbf{Summaries 2015}} & \multicolumn{2}{c}{\textbf{Descriptions 2015}} & \textbf{Best Average} \\

\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\cmidrule(lr){6-7}
\cmidrule(lr){8-9}

Training Queries  & S 2015 & D 2015 & S 2015   & D 2015  & S 2014 & D 2014 & S 2014   & D 2014  \\

% \cmidrule(lr){1-1}
% \cmidrule(lr){2-3}
% \cmidrule(lr){4-5}
% \cmidrule(lr){6-7}
% \cmidrule(lr){8-9}
% \cmidrule(lr){10-10}
\midrule

Baseline P@10 (\%)        & \multicolumn{2}{c}{31.67}          & \multicolumn{2}{c}{25}              & \multicolumn{2}{c}{38.67}     & \multicolumn{2}{c}{35}     &32.59         \\

% \cmidrule(lr){1-1}
% \cmidrule(lr){2-3}
% \cmidrule(lr){4-5}
% \cmidrule(lr){6-7}
% \cmidrule(lr){8-9}
% \cmidrule(lr){10-10}
\midrule

Linear Regression & \textit{\textbf{+7}} & +6.67             & \textit{\textbf{+8.33}}   & +7.67              	& \textit{+2.67}       	& +1.67    		& +1          & \textit{\textbf{+2.33}}   & +\textbf{5.08}  \\
AdaRank           & \textit{+6.33}       & +3.33             & \textit{+8}               & +6.67              	& \textit{\textbf{+3}}	& +1.67         	& +1.33       & \textit{\textbf{+2.33}}   & +4.92  \\
RankNet           & +6.33             	 & \textit{+6.67}    & \textit{+6}               & +6                 	& -3.67       		& \textit{+1.67}        & -2.33       & \textit{+1.67}            & +4  \\
Coordinate Ascent & \textit{+6.33}       & +6                & \textit{+8}               & +5.33              	& -3.67      		& \textit{+1}           & -3          & \textit{-0.3}             & +3.75  \\
Lambda MART	  & \textit{+4}		 & +2.33	     & +4		         & \textit{+6}	   	& -7.33			& \textit{-1.33}	& -5.33	      & \textit{-0.3}		  & +2.09  \\
MART	 	  & \textit{+5.67}	 & +2		     & \textit{+6.33}	         & +2.33		& -4.33			& \textit{-4}		& -6.77	      & \textit{-5}		  & +0.75 \\
RankBoost	  & \textit{+3.67}	 & +3		     & +4		         & \textit{+5}		& -5.67			& \textit{-3}		& -7	      & \textit{-4.67}		  & +0.25 \\
% % Random Forests    & +5.33             & -1.67             & +5.33            & +3.67              & -5          & -1.67         & -5.33       & -2.33             & 1.67  \\

\bottomrule
\end{tabular}%
}
\end{table}

Let us now analyze the results. We can definitely say that Linear Regression and AdaRank (around 5\% best average improvement)
outperform the other five methods. Linear Regression is the best for 2014 overall (+7.67\%) and
AdaRank is the best for 2015 overall (+2.67\%).

The next two methods, RankNet and Coordinate Ascent, give similarly good
improvements for 2014 (+6.33\% and +7.17\% respectively), but their performance is somewhat lower
for 2015 (only +1.67\% and +0.7\% respectively).

The bottom three methods, Lambda MART, MART and RankBoost, do not give an improvement for 2015, being worse
than the baselines by around 0.8\%, 4.5\% and 3.8\%, respectively. However, their 2014 improvements are large enough to compensate
for this, so they all give positive overall improvements.

One potential reason for the lower performance of the latter three methods might be the fact that they retrieve in the top 10
many documents for which we have no relevance judgments. Even though some of these documents \emph{might} be relevant,
they are by default considered irrelevant.
Figure \ref{unjudged-fig} shows for each learning to rank algorithm
the relation between the average P@10 improvements and the average
percentage of unjudged documents in Top 10. The top four methods
have substantially lower percentages of unjudged documents (below 8.25\%) than the bottom
three methods (above 11.25\%), which \emph{might} explain \emph{some} of the performance degradation.
Of course, we cannot
be sure if this is indeed the cause, or if simply these methods are less suited for the task at hand.

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.3]{../../ranking/plots/unjudged.png}
  }
  \caption{Absolute P@10 percent improvements vs. the number of unjudged documents in Top 10.}
  \label{unjudged-fig}
\end{figure}

Let us now decide which training sets are better.
Looking at Table \ref{l2r-res}, we see that for both Summaries 2014 and Descriptions 2015, the best
training set is \emph{generally} Summaries 2015.
For Summaries 2015, the best training set
is Summaries 2014 for the top two methods and Descriptions 2014 for the other five,
while for Descriptions 2015, the best training set is \emph{always} Descriptions 2014.
For the top two methods, therefore, summaries made better training sets than descriptions in all
four cases except Descriptions 2015.

Let us conclude the analysis of the supervised fusion results with examining the P@10 improvement per query
and intent type
for the best supervised fusion method --- Linear Regression. Figure \ref{regression-query} and Table
\ref{change-per-query-table-regression} show the change in P@10
per query and per intent type, respectively, for each of the four runs.

We can see that, similar to the unsupervised fusion methods,
Treatment queries fared better than Diagnosis and Test queries, especially for Summaries 2014 and 2015.
As discussed for the unsupervised fusion methods, this may be due to the much lower number of training
documents for Diagnosis and Test (1000 positive samples) vs Treatment (8000 positive samples).
In addition, 2014 seems much easier to improve than 2015, especially the Diagnosis and Test queries.

\begin{table}[h!]
\centering
\caption{Absolute P@10 percent improvements per intent type for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015), when using \textbf{Linear Regression}
for fusing the baseline relevance scores with the classifier scores.
The classifier is SVM with Precision@K loss.}
\label{change-per-query-table-regression}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} \\ \midrule
Diagnosis & +5                      & +11                        & +1                      & +1                         & \textbf{+4.5}             \\
Test      & +7                      & +3                         & 0                       & +3                         & \textbf{+3.25}            \\
Treatment & +9                      & +11                        & +7                      & +3                         & \textbf{+7.5}             \\ \midrule
Average   & +7		 	    & +8.33			 & +2.67		   & +2.33			& \textbf{+5.08}	 \\

\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure}[h!]
\centerline{
  \includegraphics[scale=0.215]{../../ranking/plots/bar/regression.regression.png}
  }
  \caption{Linear Regression - P@10 change per query when using SVM classifier with Precision@K loss.}
  \label{regression-query}
\end{figure}

\section{Overall Comparison between Fusion Methods}
Let us now finally conduct an overall comparison between the best fusion methods:
the three unsupervised ones (Linear Combination, RRF and Borda)
and the top three supervised ones (Linear Regression, AdaRank and RankNet).
Since for Linear Combination, both SVM classifiers with Precision@K and Recall@K
losses give improvements of at least $6\%$, we list both of them in the comparison
(Table \ref{fusion-comp}).

\begin{table}[h!]
\centering
\caption{Comparison between the best fusion methods --- absolute P@10 percent improvements for each of the four query runs
(Summaries 2014, Descriptions 2014, Summaries 2015, Descriptions 2015).}
\label{fusion-comp}
\resizebox{\textwidth}{!}{%

\begin{tabular}{@{}llllll@{}}
\toprule
  & \textbf{Summaries 2014} & \textbf{Descriptions 2014} & \textbf{Summaries 2015} & \textbf{Descriptions 2015} & \textbf{Average} 	\\\midrule
Baseline P@10 (\%)                	& 31.67     	& 25  		& 38.67          	& 35        	&   32.59      		\\ \midrule
Lin Comb - Prec@K		& +8          	& +9     	& \textbf{+4}		& \textbf{+3.33}& \textbf{+6.08}    	\\
Lin Comb - Rec@K		& \textbf{+8.33}& \textbf{+10}	& +3			& +2.67		& \textbf{+6}		\\
Lin Regression			& +7		& +8.33		& +2.67		   	& +2.33		& \textbf{+5.08}	 \\
AdaRank				& +6.33		& +8		& +3		   	& +2.33		& \textbf{+4.92}	 \\
RRF                             & +5         	& +7.33    	& +3.67            	& +3  		& \textbf{+4.75}        \\
Borda				& +4.67  	& +8.67     	& +2.33           	& +1.33    	& +\textbf{4.25} 	\\
RankNet				& +6.67		& +6		& +1.67			& +1.67		& \textbf{+4}		\\
\bottomrule
\end{tabular}
}
\end{table}

The best method overall is therefore Linear Combination using an SVM classifier. For 2014, the best loss function
is Recall@K, while for 2015 the best loss function is Precision@K.

Compared to the unsupervised fusion methods, both Linear Regression (+5.08\%) and AdaRank (+4.92\%) sit between
Linear Combination (+6\%) and RRF (+4.75\%). Of course, one advantage of the learning-to-rank algorithms
is that we do not need to manually search for the best weight $\lambda$ between the two features,
since it is automatically determined
from the training data.
We do need, however, training data and to search for the best hyper-parameters.
Thus, there are trade-offs for each approach (supervised vs. unsupervised),
but nevertheless they each give solid average improvements of at least 4-5\% over the baseline.

% \begin{table}[h!]
% \caption{Learning to Rank Percentage of unjudged documents in Top 10.}
% \label{l2r-res}
% \resizebox{1.2\textwidth}{!}{%
% \begin{tabular}{@{}l|ll|ll|ll|ll|l@{}}
% \toprule
% Training Queries  & Summ. 2015 & Desc. 2015 & Summ. 2015   & Desc. 2015  & Summ. 2014 & Desc. 2014 & Summ. 2014   & Desc. 2014  \\ \midrule
%
% Test Queries      & \multicolumn{2}{c|}{Summaries 2014} & \multicolumn{2}{c|}{Descriptions 2014} & \multicolumn{2}{c|}{Summaries 2015} & \multicolumn{2}{c|}{Descriptions 2015} & Best Average \\ \midrule
%
% Linear Regression & 4              & 5                 & 10               & 12                 & 4              & 1                 & 8                & 7        & 6.25          \\
% AdaRank           & 4              & 4                 & 12               & 10                 & 1              & 1                 & 9                & 7        & 6         \\
% RankNet           & 9              & 8                 & 17               & 17                 & 9              & 1                 & 11               & 7        & 8.25         \\
% Coordinate Ascent & 4              & 7                 & 12               & 16                 & 9              & 5                 & 12               & 8        & 7.25         \\
% Lambda MART       & 11             & 9                 & 17               & 19                 & 11             & 4                 & 16               & 11       & 11.25         \\
% MART              & 9              & 1                 & 15               & 19                 & 9              & 8                 & 14               & 15       & 11.75       \\
% RankBoost         & 11             & 12                & 19               & 20                 & 11             & 8                 & 16               & 13       & 13       \\
%
% \bottomrule
% \end{tabular}%
% }
% \end{table}

\section{Comparison with Previous Research}
We conclude this section by comparing our findings to previous research in the TREC CDS area.
As discussed in Section \ref{related}, the most successful previous research using query intent types for reranking
was conducted by Choi et al.~\cite{choi} for TREC 2014. However, they did not write a paper for TREC 2015,
which means that we can only compare the 2014 results.

Recall that \cite{choi} uses a similar idea as ours, namely fusing baseline scores and
classifier scores. They also use the Clinical Hedges Database for training and SVM for classification.
Their SVM uses ROC-area loss and their fusion uses Borda with a weight $\lambda=0.5$.
Like us, they also use the diagnosis samples for the test classifier.
The runs that they re-rank are the \emph{ExternalQE} runs, where
they also use externally-tagged knowledge-based query expansion in addition to the baseline similarity score
(runs \emph{Baseline}).

Let us now list the runs involved in the comparison:
\begin{itemize}
 \item from \cite{choi}, runs \emph{SNUMedinfo1} and \emph{SNUMedinfo4} for Summaries 2014 and Descriptions 2014, respectively.
  In these runs --- like in our system --- all queries are reranked;
 \item from \cite{choi}, runs \emph{SNUMedinfo3} and \emph{SNUMedinfo6} where the Diagnosis queries are \emph{not} re-ranked;
 \item our results using the approach from \cite{choi} --- Borda Fusion, SVM classifier with ROC-Area loss and $\lambda=0.5$;
 \item our results using the best unsupervised fusion method for 2014 --- Linear Combination, SVM classifier with Recall@K loss and $\lambda=0.7$;
 \item our results using the best supervised fusion method for 2014 --- Linear Regression trained on the Summaries 2015 queries.
\end{itemize}

Table \ref{choi-comp} summarizes the results of the comparison.
Before delving into the discussion, let us mention that since we and \cite{choi} have different baselines,
we cannot do a real, unequivocal comparison. Different baselines imply different potentials for improvement, and
not even having the same P@10 is enough, as exemplified by the difference in improvements for Summaries
2014 vs. Descriptions 2014 for \cite{choi} (both baselines are 32\%, but the improvements are different).
We nevertheless do the comparison in order to see if we can recommend other approaches that \emph{seem}
to perform better than the ones in \cite{choi}.

\begin{table}[h!]
\centering
\caption{Comparison with previous research --- absolute P@10 percent improvements for Summaries 2014 and Descriptions 2014.}
\label{choi-comp}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
                          & \textbf{Summaries 2014} & \textbf{Descriptions 2014} \\\midrule
Baseline from \cite{choi} P@10 (\%) 		& 32     			& 32                         \\\midrule
Improvements from \cite{choi}
- diagnosis reranked  				& +1.67  			& +1                          \\
Improvements from \cite{choi}
- diagnosis not reranked 			& +2.67   			& +4.67                       \\\midrule
%                           &                         &                            \\
Our Baseline P@10 (\%)            			& 31.67                   	& 25                         \\\midrule
Our improvements - method from \cite{choi} 	& +4.67                    	& +7                          \\
Our improvements - best unsupervised fusion 	& +8.33                    	& +10                         \\
Our improvements - best supervised fusion     	& +7                       	& +8.33                      \\\bottomrule
\end{tabular}%
}
\end{table}

First, let us observe that our and \cite{choi}'s Summaries baselines have almost equal P@10 (32\% and 31.67\%),
but the Descriptions baselines are very different (ours is 25\% and \cite{choi}'s is 32\%).
This might be due to the fact that they also performed externally-tagged knowledge-based query expansion
and possibly different data pre-processing for the
run that we consider as baseline.

Second, for \cite{choi}, re-ranking the Diagnosis queries is worse than \emph{not} re-ranking them, which means that
their system did not work well for the Diagnosis queries and made them worse.
Our system, however, improves all three types of queries
(see Tables \ref{change-per-query-table-lincomb} and \ref{change-per-query-table-regression}),
even though the Diagnosis and Test improvements are indeed lower than the Treatment ones.

Third, the absolute improvements are higher when using our methods.
While the maximum improvement
in \cite{choi} is +2.67\% for Summaries and +4.67\% for Descriptions (both in the runs not reranking Diagnosis queries),
our improvements are consistently
higher than that --- at least +7\% for Summaries and at least +8.33\% for Descriptions.

As a result, our best P@10s for Summaries are 40\% and 38.67\%
(unsupervised and supervised, respectively) compared to the best Summaries from~\cite{choi}, namely 34.67\%.

For Descriptions, it is more difficult to compare, since our baseline has a much lower P@10.
Still, our reranked P@10s for Descriptions are 35\% and 33.33\% (unsupervised and supervised, respectively)
 --- better than \cite{choi}'s P@10 of 33\% when reranking all three categories.
Our Descriptions P@10s are, however, slightly worse
than \cite{choi}'s P@10 of 36.67\% when reranking only the Test and Treatment queries, but keep in mind that:
1) we started with a baseline 7\% worse than theirs --- the absolute improvements themselves being much higher, +10\% and +8.33\%
vs +4.67\% ---, and 2) 13\% percent of the documents in Top 10 are unjudged for Linear Combination,
some of which \emph{may} potentially be relevant.

To sum up all of the above, we can safely recommend our best supervised and unsupervised fusion methods
(Linear Regression and Linear Combination) as well as the next-best ones (AdaRank and RRF),
together with SVM with Precision@K or Recall@K losses,
as solid alternatives to previous approaches for the TREC Clinical Decision Support track.

% \section{Random clf}
% 32.78 (31.58 25.59 38.91 35.05)
% +0.19 (-0.09 +0.59 +0.24 +0.05)

% \begin{verbatim}
%  >>> y=[1,0,0,1,0,0,1,0,0,1]
% >>> stats.ttest_rel(x,y)
% Ttest_relResult(statistic=-2.4494897427831783, pvalue=0.036787497879786128)
% >>> y=[1,0,0,1,0,0,3,0,0,1]
% >>> stats.ttest_rel(x,y)
% Ttest_relResult(statistic=-1.9639610121239315, pvalue=0.081126188845840497)
% >>> y=[1,0,0,1,0,0,5,0,0,1]
% >>> stats.ttest_rel(x,y)
% Ttest_relResult(statistic=-1.6329931618554521, pvalue=0.13690412558075216)
%
% \end{verbatim}








