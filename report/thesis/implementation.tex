\chapter{Implementation}\label{impl}

In this chapter, we discuss the implementation of the system formally defined in the previous section. We first
introduce the data we are working on, then move on to the implementation of the similarity and type classifier scoring functions,
and then conclude with the implementation of the various unsupervised and supervised fusion methods.

\section{Data}
For our use case, we use the data from the TREC 2014 and 2015 Clinical Decision Support tracks \cite{trec}. The data consists of:
\begin{itemize}
 \item a list of topics;
 \item a document collection;
 \item relevance judgments (\emph{qrels}) for several topic-document pairs.
\end{itemize}

\subsection{Topics}
A TREC CDS \emph{topic} consists of an $(\textsf{ID}, \textsf{Description}, \textsf{Summary}, \textsf{Type})$ tuple, where:
\begin{itemize}
 \item \textsf{ID} is an integer that uniquely identifies the topic for that year;
 \item \textsf{Description} is the medical case report, containing detailed information about
 the patient's symptoms, medical history, vital statistics and any test results. It is verbose and written in natural language;
 \item \textsf{Summary} is the shorter version of the case report, containing only the most relevant information;
 \item \textsf{Type} is the intent type, that is, the generic clinical question the physician is interested in
  (diagnosis, test or treatment).
\end{itemize}

We have 30 topics from TREC 2014 CDS track and 30 topics from TREC 2015 CDS track.
In both years, 2014 and 2015, topics 1--10 have type \emph{diagnosis}, topics 11--20 have type \emph{test}
and topics 21--30 have type \emph{treatment}.
Topics are provided in XML format, for example:

\begin{Verbatim}[fontsize=\small]
  <topic number="1" type="diagnosis">
    <description>
      A 58-year-old African-American woman presents to the ER
      with episodic pressing/burning anterior chest pain that began two
      days earlier for the first time in her life. The pain started while
      she was walking, radiates to the back, and is accompanied by nausea,
      diaphoresis and mild dyspnea, but is not increased on inspiration.
      The latest episode of pain ended half an hour prior to her arrival.
      She is known to have hypertension and obesity. She denies smoking,
      diabetes, hypercholesterolemia, or a family history of heart disease.
      She currently takes no medications. Physical examination is normal.
      The EKG shows nonspecific changes.
    </description>
    <summary>
      58-year-old woman with hypertension and obesity presents with
      exercise-related episodic chest pain radiating to the back.
    </summary>
  </topic>
\end{Verbatim}

\subsection{Document Collection}
We are provided with approximately 733,100 medical articles from the Open Access subset of PubMedCentral \cite{oa}.
The articles are in NXML format
and contain the article's title, abstract (if any) and full text.
However, some articles ($\sim$ 50,000) do not include the full text
as NXML but as a scanned PDF image.
For these articles, we use the Open Access FTP Service \cite{oa-ftp} to fetch their plain text contents.

\subsection{Relevance Judgments}\label{qrels}
We have relevance judgments for approximately 38,000 topic-document pairs for 2014 and 2015 each, with about 10\% of them
being positive (relevant). The relevance judgments were assigned by medical experts after each edition of TREC CDS.

Since not all 733,100 $\times$ 60 topic-document pairs could have possibly been evaluated, it sometimes happens that our system retrieves documents
that are not judged, and by default are considered non-relevant. This \emph{may} negatively impact the performance evaluation of
the system, since part of these documents may actually be relevant.

\section{Scoring Function Implementation}
We now present the implementation of the scoring Function \ref{fusion}. Recall that the function has three components:
the baseline relevance score, the type classifier score and the fusion of scores.

\subsection{Relevance Scoring Implementation}\label{sim-impl}
As discussed in Section \ref{bm25-formula}, we use the \textsf{BM25} scoring function to compute relevance scores between the query
text and documents. To this end, we employ the Lucene information retrieval system \cite{lucene}, which works in two main stages:
\begin{enumerate*}[label=\arabic*)]
 \item \emph{indexing} the document collection;
 \item running the \emph{search queries} on the index.
\end{enumerate*}

We index the document collection as follows:
\begin{enumerate}[label=\arabic*)]
 \item we parse each NXML file into plain text (article title, abstract and body), using Java's DOM parser.
  If an NXML file is missing the full article text, we parse it from the plain text version of the Open-Access collection instead;
 \item we send the parsed text to Lucene for indexing,
    using the BM25 Similarity and the English Analyzer for tokenization and stemming.
\end{enumerate}

At query time, we parse the XML file containing the topics, and convert the topic text to Lucene queries.
Since each topic has both a summary and a description, we can use either of them as the query text.
We therefore
generate four sets of query \emph{runs}: using the summaries from 2014, using the descriptions from 2014, using the summaries from 2015 and,
lastly, using the descriptions from 2015.
For each run, we retrieve the top $N=100$ results per query
(using the same configuration as for indexing --- \texttt{BM25 Similarity} and \texttt{English Analyzer}),
thereby producing four sets of \emph{baseline} relevance scores.
These scores are then fused with the type classifier scores, discussed next.

\subsection{Intent Type Scoring Implementation}
Computing intent type scores involves only the document collection and the three query intent types (diagnosis, test and treatment);
we do not need the queries' text for this part.

Furthermore, we do not use the documents in their original form,
but we tokenize, lowercase and stem them using Lucene's \texttt{EnglishAnalyzer}.

Recall from Formula \ref{clf-combine-formula} that the final intent type classifier score is a linear combination of the
machine-learning classifier score and the keyword-counter score. Let us now discuss their implementation.

\subsubsection{Keyword Counter Implementation}
For the keyword counter, we use Formula \ref{basic-formula}, with the following keywords sets per intent type:
\begin{itemize}
 \item for diagnosis, we use \{\emph{diag}\} --- the stem of terms like \emph{diagnosis}, \emph{diagnostic}, \emph{diagnose} etc;
 \item for test, we use \{\emph{diag}, \emph{test}\} --- the stems of terms like \emph{diagnosis}, \emph{diagnostic}, \emph{diagnose} and
 \emph{test}, \emph{testing} etc;
 \item for treatment, we use \{\emph{treat}\} --- the stem of terms like \emph{treat}, \emph{treatment}, \emph{treated} etc.
\end{itemize}

\subsubsection{Machine-Learning Classifier Implementation}
\paragraph{Training Data}
For the machine-learning classifier, we need training data for each of the three intent types (diagnosis, test and treatment).
For this, we use the Clinical Hedges Database (CHD)
\cite{chd},
which has diagnosis labels for around 1100 documents, and treatment labels for around 8000 documents. It does
not have any test labels, but we can use the diagnosis labels for test, since the two intent types are relatively similar
(this was also the approach taken by \cite{choi}).
The CHD also has 37,000 documents of other types, which we use as \emph{negative} samples.
One drawback of the CHD, however, is that the training documents are not part of the Open-Access subset, so their full text is not
available for (programmatic) download. Therefore, we only use the titles and the abstracts for training.

\paragraph{Classifier Implementation}
We use \texttt{scikit-learn} \cite{scikit} for the SVM-based classifiers with hinge,
squared hinge, $\epsilon$-insensitive and squared losses, as well as for the Ridge, Logistic, Perceptron,
Passive Aggressive, Naive Bayes and Multilayer Perceptron classifiers.
We also use \texttt{scikit-learn} to compute the TFIDF-based document features.

We use \texttt{SVM-perf} \cite{svmperf3,svmperf1,svmperf2}
for the SVM-based classifiers with zero/one, error-rate, $F_1$, Precision@K, Recall@K and ROC-area losses.

For the convolutional neural networks, we use the \texttt{gensim} package \cite{gensim} to learn the \texttt{word2vec}
word embeddings (trained over the entire document collection)
and the \texttt{tensorflow} package \cite{tf} for training the neural networks, based on the construction
from \cite{cnn} and \cite{cnn-code}, with an additional hidden layer before the last (fully connected) layer.

The classifier scores are given by the (confidence) values of the decision function, or the predicted probabilities,
when available (e.g., for Naive Bayes).

\subsection{Fusion of Relevance and Intent Type Scores}
Now that we have computed the relevance and type classifier scores, we can finally fuse them together,
as described in Section \ref{fusion-theory}.
% we investigate both unsupervised as well as supervised fusion methods.
% Before doing that, however, we normalize them to the $[0,1]$ interval, as described in Section \ref{unsup-fusion-theory}.

\subsubsection{Unsupervised Fusion}\label{impl-unsup}
For each of the three unsupervised fusion methods discussed in Section \ref{unsup-fusion-theory}
--- Linear Combination, RRF and Borda --- we vary the weight $\lambda$ from 0 to 1 in steps of 0.01,
according to Equations \ref{interp-formula}, \ref{rrf-formula} and \ref{borda-formula}, respectively.
% For each weight and for each query, we fuse the BM25 relevance
% scores with the type classifier scores, for each document in the top $N=100$.
Based on the experiments from Chapter \ref{exp}, we then decide
which weights yield the highest improvements for each fusion method.

\subsubsection{Supervised Fusion}\label{impl-sup}
For the supervised fusion methods discussed in Section \ref{sup-fusion},
we use the \texttt{RankLib} package from the \texttt{Lemur} project \cite{ranklib}.

For training, we use the query-document pairs with relevance judgments from one year (e.g., 2014),
and for testing, we use the query-document pairs from the baseline results of the \emph{other} year (e.g., 2015).

The features are the BM25 relevance scores and the type classifier scores, as discussed in Section \ref{sup-fusion}.

Now that our system is fully implemented, we move forward to the Experimental Results Chapter.




