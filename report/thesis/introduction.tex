\chapter{Introduction}
Given the vast amount of medical literature available nowadays (more than 15 million articles indexed by PubMed)
and the accelerating rate at which new articles are published (at least 500,000 in each of the previous ten years),
it is becoming increasingly difficult and time-consuming for physicians to filter the medical information
relevant to their patients' cases.

\emph{Clinical Decision Support} (CDS) aims to address part of this problem by using \emph{artificial intelligence}
to assist physicians with their clinical information and decision-making needs.
Since this is a very active area of research,
the Text Retrieval Conference (TREC)
introduced in 2014 the CDS track,
whose goal is to simulate real-world medical case scenarios and to gather world-wide state-of-the-art research in the area.

The TREC CDS track focuses on situations where physicians have the \emph{case report} of a patient
--- containing current symptoms, medical history, test results and any other medical information ---
and need assistance with either one of these three generic
\emph{clinical questions}:
\begin{itemize}[noitemsep,nolistsep]
 \item what is the patient's \emph{diagnosis} ?
 \item what \emph{tests} should be performed ?
 \item how should the patient be \emph{treated} ?\end{itemize}

The purpose of the track is to build an \emph{information retrieval} (IR) system that
retrieves \emph{medical articles} relevant to both the medical
case report and to the generic clinical question the physician is interested in.

In more formal, IR, terms, we are asked to retrieve \emph{documents} for \emph{queries} that have two components:
\begin{itemize}[noitemsep, nolistsep]
 \item a \emph{textual} component --- the medical case report;
 \item an \emph{intent type} component --- one of the three clinical questions above.
\end{itemize}

Both query components are taken into account when evaluating document \emph{relevance}.
This means that it is not enough to only use the medical case report
for retrieval, since the document might not answer the generic clinical question posed by the physician.
The \emph{challenge} is, therefore, not only retrieving medical articles for queries written in a highly-specialized and technical
vocabulary, but also finding ways of incorporating the \emph{intent type} information into the process.

Yet, most participants from TREC CDS 2014 and 2015 ignored the intent types,
and only used the query text for retrieval.
Most of their efforts revolved around making the query more meaningful, with techniques
such as query expansion, pseudo-relevance feedback, expansion using medical terms from the UMLS, MeSH or MetaMap databases and
fine-tuning the scoring function of their IR systems.

A few participants did, nonetheless, incorporate intent type information into the retrieval process. Most of them
 expanded the query using synonyms of words such as diagnosis, test and treatment, depending on the query type.

A more elaborate idea was using a machine-learning \emph{classifier} to categorize the retrieved documents according
to the three intent types and \emph{fuse} the classification scores
with textual similarity scores in order to improve the overall retrieval performance.
The approach proved successful in some cases, leading to improvements compared to only using textual similarity.

However, TREC generally limits the number of submissions per team, which, in turn, restricts
the number of configurations that participants can choose for their systems.
Indeed, as it turned out to be the case for systems using this approach,
they only attempted one type of classifier (e.g., Support Vector Machines)
and one type of fusion method (e.g., Borda).

This means that even though the idea of using classification and score fusion
for Clinical Decision Support looks promising,
the amount of research carried out so far has been rather limited and fragmentary.

The goal of this paper is, therefore, to
explore this idea in a much more extensive and systematic way, both in breadth --- by experimenting
with a wider range of machine-learning classifiers and fusion methods ---,
and in depth --- by performing a more comprehensive analysis of the resulting models.

In order to do this, we use a variety of classifiers, ranging from classical ones, like Support Vector Machines,
Logistic Regression, Ridge, Passive Aggressive, Naive Bayes, Perceptrons,
to more modern, deep-learning ones, like Convolutional Neural Networks and Multilayer Perceptrons.
For fusing textual and intent type relevance scores,
we experiment with both unsupervised fusion methods, such as Weighted Linear Combination, Reciprocal Rank Fusion and Borda Fusion,
as well as with supervised methods based
on Learning-to-Rank algorithms such as AdaRank, RankNet, Regression, Coordinate Ascent, MART, Lambda MART and RankBoost.

Our goal is to design and implement a framework that allows us
to explore this idea as much as possible, investigate all of the methods above,
analyze their performance in detail,
determine which ones work best and compare them with existing research.
We want to analyze each approach in depth and determine its advantages, disadvantages and trade-offs.

The most significant advantage that we have compared to previous participants in the TREC CDS editions is
that we have \emph{more data}. Previous participants from 2014 had no prior data, while the ones from 2015
only had the 2014 data. This limitation hampered their ability to properly train and evaluate their models, since they
had too little data to make a definitive judgment.

We, however, have both the 2014 and 2015 data, which means that
we have more queries and more relevance judgments.
Even so, we still do not have as many as we would like in order to be able to give an unequivocal answer.
For example, we will find
that some parameters that work very well for 2014 are sub-optimal for 2015 and vice-versa.
Having two
years to evaluate our models allows us to choose configurations that are more \emph{robust} and work better \emph{on average}, but we
may still be a certain way off from the optimal configuration.

Having data from two years also
enables us to use supervised learning-to-rank algorithms,
since we can use the data from one year for training and the data from the other year for testing,
something that could not have been done for TREC 2014 or 2015.

The main contribution of our work is, therefore, investigating a very broad range of methods of incorporating
intent type information into the retrieval process,
facilitated by us having more data that enables us to evaluate the resulting models better and more confidently.

We evaluate our systems on both the 2014 and the 2015 queries; we use textual similarity
as a \emph{baseline} and then examine how much each of our approaches can potentially \emph{improve} upon this baseline.
We investigate what the best parameters are, how each method behaves for the three
intent types and what the improvements for each query are.

After implementing all of these approaches and performing an in-depth analysis of their results,
we are confident that we can make a recommendation.
Our best approaches improve the 2014 baseline Precision@10 by up to 8\%, yielding an overall
score better than all participating systems from TREC 2014,
and the 2015 baseline by up to 4\%, yielding a score better than all participating
systems from TREC 2015 that used machine-learning classifiers to incorporate intent
types into the retrieval process.

The next chapters detail how we build our system and evaluate our results.
We begin by researching the previous work from the CDS area in Chapter \ref{related},
with a strong focus on the papers from the TREC CDS track.
In Chapter \ref{meth}, we give a formal definition of our retrieval system, discussing the ways in which we classify documents
and incorporate intent types into the ranking process.
In Chapter \ref{impl}, we present the practical realization of our retrieval system, including any external libraries
and frameworks that we use.
In Chapter \ref{exp}, we apply our system on the TREC 2014 and 2015 data, and evaluate each of our models.
We conclude by making a comparison between our best methods and previous research.







